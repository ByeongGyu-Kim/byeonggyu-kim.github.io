---
title: "Global Optimization"
date: 2025-05-25 18:00:00 +0900
categories:
  - Imaging Photography
tags:
  - Vision
description: 
toc: true
comments: false
cdn: 
image:
math: true
pin: false
mermaid: false
---

## Global Optimization
Image processing에서 optimization에 대한 내용을 우선 다루고자 한다. 
최적화 문제는 보통 아래와 같은 objective function을 풀어서 x라는 해를 찾아야 한다. x는 이 function f(x)를 최소로 만드는 해이며, 추가적인 constraint가 존재하는 경우에는 크게 두 종류로 나타낼 수 있다.

$$
x^* = \arg\min_x f(x)
$$

$$
\text{subject to} \quad g_i(x) \leq 0, \quad i = 1, \dots, m
$$
    (inequality constraints)

$$
h_i(x) = 0, \quad i = 1, \dots, p
$$
    (equality constraints)

그럼 이러한 최적화 문제를 어떻게 활용할 것인지가 중요한 문제이다. 지금까지는 어떤 input 이미지가 주어진다면 warping, filtering 등을 이용해서 이미지를 변형시키는데 초점을 맞추었다.
그렇다면 위에서 살펴본 최적화 문제는 왜 필요한 것일까?

## Global Optimization in Image
### 📌 1. 이미지 처리 문제는 대부분 역문제 (inverse problem)
이미지 처리에서 우리가 관심 있는 문제는 다음과 같은 형태를 가진다.
- 이미지 디노이징: 노이즈가 섞인 이미지에서 원래 깨끗한 이미지를 복원
- 슈퍼 해상도: 저해상도 이미지에서 고해상도 이미지 복원
- 이미지 분할: 각 픽셀이 어떤 객체에 속하는지 결정
- 광원 추정, 포즈 추정 등등

➡️ 모두 **관측된 결과(이미지)**로부터 원인을 유추하는 문제 = 역문제
➡️ 이들은 정보 부족, 노이즈, 모호성 때문에 해가 하나로 정해지지 않는다.
➡️ 그래서 어떤 기준(목표 함수)을 세우고 그걸 최적화해야만 합리적인 해를 찾을 수 있다는 것이다.

예를 들어 Image denoising을 생각해보자. 
노이즈가 섞인 이미지 $$g$$가 주어졌을 때, 우리는 원래의 깨끗한 이미지를 복원하는 것을 목표로 한다.  
이러한 문제는 일반적으로 다음과 같은 에너지 최소화 문제로 정의할 수 있다.

$$
f^* = \arg\min_f \left[ \mathcal{E}_d(f) + \lambda \mathcal{E}_s(f) \right]
$$

여기서 $$\mathcal{E}_d(f)$$는 데이터 항(data fidelity term)으로, 복원된 이미지 $$f$$가 입력 이미지 $$g$$와 유사해야 한다는 조건을 의미한다.

$$
\mathcal{E}_d(f) = \sum_{x,y} (f(x,y) - g(x,y))^2
$$

또한, $$\mathcal{E}_s(f)$$는 정규화 항(regularization term)으로, 복원된 이미지가 부드러운(smooth) 특성을 가지도록 유도한다. 일반적으로는 그래디언트의 크기를 최소화하는 방식으로 정의된다:

$$
\mathcal{E}_s(f) = \sum_{x,y} \|\nabla f(x,y)\|^2
$$

이 두 항의 균형을 조절하는 $$\lambda$$는 하이퍼파라미터로서, $$\lambda$$가 크면 더 부드러운 이미지를 유도하고, 작으면 입력 이미지와의 유사성을 더 강조한다.
따라서 이미지 디노이징 문제는 노이즈를 제거하면서도 원래의 이미지 구조를 최대한 보존하는 방향으로 최적화하는 문제로 귀결된다.

선형 회귀 문제를 통해 Least-squares method를 살펴보자.  
주어진 데이터 샘플 $(x_i, y_i)$들이 있을 때, 우리는 이 데이터들을 가장 잘 설명하는 직선을 찾고자 한다.  
즉, 선형 모델 $f(x) = ax + b$를 가정할 때, 다음과 같은 조건을 만족하는 계수 $a, b$를 찾는 것이 목표이다:

$$
y_i \approx f(x_i) = ax_i + b
$$

이때 오차 제곱합을 최소화하는 것이 Least-squares 방법이며, 최적화 문제는 다음과 같이 정의된다:

$$
\min_{a, b} \sum_{i} (y_i - (ax_i + b))^2
$$

## Two Approaches to Global Optimization
지금부터는 Global Optimization을 해결하기 위한 두 가지 접근법에 대해 살펴보겠다.
두 가지 접근법은 각각의 문제에 따라 다르게 적용될 수 있으며, 최적화 문제를 해결하는 데 있어 중요한 역할을 하고, 서로 밀접한 연관이 있다.

### 1. Regularization or Variational methods
이 접근법은 최적화 문제를 변분 문제로 변환하여 해결하는 방법이다.

### 2. Bayesian statistics
이 접근법은 확률론적 모델링을 통해 최적화 문제를 해결하는 방법이다. Bayesian statistics는 사전 확률(prior)과 우도(likelihood)를 결합하여 사후 확률(posterior)을 계산하고, 이를 통해 최적의 해를 찾는다.

