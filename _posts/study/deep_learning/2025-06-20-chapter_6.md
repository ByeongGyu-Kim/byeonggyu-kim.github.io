---
title: "Deep learning Chapter 6 - Deep Neural Networks"
date: 2025-05-22 18:00:00 +0900
categories:
  - Deep Learning
tags:
  - Machine Learning
description: 
toc: true
comments: false
cdn: 
image:
math: true
pin: false
mermaid: false
---

본 글은 **"Deep Learning: Foundations and Concepts"** (저자: Christopher Bishop)의 6장, Single layer Networks: Regression 내용을 한국어로 정리한 것입니다.

## 0. Deep Neural Network의 등장

최근 몇 년간 **딥 뉴럴 네트워크(Deep Neural Networks, DNNs)**는 머신러닝의 핵심 기술로 자리잡았다. 음성 인식, 이미지 분류, 자연어 처리, 자율주행, 의료 진단 등 다양한 분야에서 뛰어난 성능을 보이며 실질적인 성과를 만들어내고 있다. 과거에는 입력과 출력간의 관계를 직선으로 근사하는 선형 회귀나 다항식 혹은 사인 함수 등과 같이 미리 정해놓고 모델을 구성하는 고정된 basis function을 사용하는 모델이 주로 사용되었다. 하지만 이러한 모델은 특정 문제에서는 잘 작동하나 일반적으로 고차원 입력을 가진 문제나 복잡한 비선형 관계가 존재하는 문제에서는 한계를 드러내게 되었다.

DNN이 필요한 이유가 뭘까? 예를 들어, 다음과 같은 문제를 생각해보자.

> 📸 **문제:** 손글씨 이미지가 숫자 ‘3’인지 아닌지를 판별하라.

이 이미지는 28×28 픽셀로 이루어져 있으며, 이는 곧 784차원의 벡터와 같다. 단순히 픽셀 값을 선형 결합하는 방식으로는 다양한 위치, 기울기, 두께를 가진 ‘3’을 올바르게 인식하기 어렵다.

> 📌 하지만 DNN은 이를 해결할 수 있다.  
> 초층에서는 **선**, **모서리**, **곡선** 등의 저수준 특징을 추출하고,  
> 중간 층에서는 **모양**, **위치** 등의 조합을 인식하며,  
> 마지막 층에서는 "이것은 숫자 3이다"라는 고수준 개념으로 이어진다.

이러한 계층적 구조는 단층 모델(shallow model)로는 도달할 수 없는 표현력을 제공한다. 본 6장에서는 다음과 같은 질문들에 답하고자 하며 지금부터 자세히 이야기를 나누어보도록 하겠다.

- 고정된 basis function을 사용하는 선형 모델이 고차원에서 왜 한계가 있는가?
- basis function을 데이터에 맞게 학습하려면 어떻게 해야 하는가?
- 다층 구조를 갖는 MLP는 어떻게 작동하며 왜 강력한가?
- 딥러닝은 어떻게 데이터의 복잡한 패턴을 자동으로 학습하는가?

---
## 1. Limitations of Fixed Basis Functions

### 6.1.1 Curse of Dimensionality

선형 회귀 모델은 입력에 대해 고정된 basis function을 적용하고, 이를 선형 결합함으로써 출력 값을 예측하는 방식이다. 예를 들어, 하나의 입력 변수에 대해 다항식 회귀를 적용하면 다음과 같은 식을 얻을 수 있다.

$$
y(x, w) = w_0 + w_1x + w_2x^2 + \cdots + w_Mx^M
$$

이제 입력 변수가 하나가 아니라 \( D \)개라고 가정해보자. 이 경우, 모든 변수의 상호작용을 반영하기 위해 3차 다항식을 구성하면 다음과 같은 항들이 포함된다.

$$
y(\mathbf{x}, w) = w_0 + \sum_{i=1}^D w_i x_i + \sum_{i=1}^D \sum_{j=1}^D w_{ij} x_i x_j + \sum_{i=1}^D \sum_{j=1}^D \sum_{k=1}^D w_{ijk} x_i x_j x_k
$$

이처럼 차수가 올라갈수록 파라미터의 수는 \( \mathcal{O}(D^M) \)에 비례하여 기하급수적으로 증가한다. 이는 곧 **차원의 저주(curse of dimensionality)**로 이어진다. 차원이 높아질수록 필요한 파라미터의 수는 폭발적으로 늘어나며, 이는 학습 시간, 메모리 사용량, 데이터 요구량의 급격한 증가를 야기한다.

단순한 셀 분할 방식도 이 문제를 피하지 못한다. 입력 공간을 일정한 간격의 셀로 나누고, 각 셀에 속한 학습 데이터의 다수 클래스를 새로운 입력에 할당하는 방식은, 낮은 차원에서는 그럭저럭 동작하지만 차원이 증가하면 대부분의 셀이 비게 되어 무의미해진다.

> 📌 **[그림 삽입: Figure 6.3]**  
> 차원이 높아질수록 격자 셀의 개수가 기하급수적으로 증가함을 보여주는 도식

---

## 6.1.2 High-Dimensional Spaces

고차원 공간에서는 우리의 직관이 무력해진다. 예를 들어, 반지름 1인 구(sphere)가 있다고 하자. 이때, 구의 가장자리(표면)에 가까운 얇은 영역의 부피가 전체 부피에서 차지하는 비율은 다음과 같이 계산된다:

$$
\text{Volume fraction} = 1 - (1 - \epsilon)^D
$$

이 값은 \( D \)가 커질수록 1에 가까워진다. 즉, 대부분의 부피가 중심이 아닌 바깥쪽 껍질에 몰려 있게 된다.

> 📌 **[그림 삽입: Figure 6.4]**  
> 고차원 구에서 반지름 \( r = 1 - \epsilon \)과 \( r = 1 \) 사이에 해당하는 부피 비율 그래프

이와 유사하게, 고차원 Gaussian 분포 또한 중심에 몰리지 않고 특정 반지름 주변의 얇은 구간에 대부분의 확률 질량이 집중된다. 이는 고차원 공간에서 **"가까움"**이라는 개념이 모호해지고, 많은 머신러닝 알고리즘의 기본 전제가 성립하지 않게 됨을 의미한다.

> 📌 **[그림 삽입: Figure 6.5]**  
> 고차원 Gaussian 분포의 반지름에 따른 확률 밀도 분포

---

## 6.1.3 Data Manifolds

그렇다면 현실의 데이터는 정말로 고차원 전체 공간에 흩어져 있는가? 실제로는 그렇지 않다. 대부분의 고차원 데이터는 **훨씬 더 낮은 차원의 다양체(manifold)** 위에 위치한다. 예를 들어, 손글씨 숫자 ‘2’가 다양한 위치, 기울기, 회전으로 작성된 이미지를 생각해보자. 각 이미지는 수천 개의 픽셀로 표현되므로 고차원 벡터지만, 그 차이의 원인은 사실상 **세 가지 자유도(위치 2개 + 회전)**로 설명할 수 있다. 즉, 이 데이터는 본질적으로 **3차원 비선형 다양체** 위에 존재한다.

> 📌 **[그림 삽입: Figure 6.7]**  
> 다양한 위치와 회전을 가진 손글씨 숫자 ‘2’의 예시

이와 달리, 각 픽셀을 무작위로 채운 이미지는 어떨까? 표면적으로는 같은 차원을 가지지만, 무작위 이미지는 전혀 자연스럽지 않다. 자연 이미지에는 강한 **픽셀 간의 상관관계**가 존재하기 때문이다.

> 📌 **[그림 삽입: Figure 6.8]**  
> 위: 실제 자연 이미지 / 아래: 무작위 RGB 픽셀로 생성된 이미지

이러한 관찰은 딥러닝이 왜 효과적인지를 설명하는 데 핵심적이다. 현실 세계의 데이터는 고차원처럼 보일 수 있지만, 실제로는 특정 구조나 규칙이 존재하는 저차원 공간에 위치한다. **딥 뉴럴 네트워크는 이 manifold 구조를 학습하고, 그 위에서 동작할 수 있도록 basis function을 최적화함으로써 기존 모델의 한계를 극복한다.**
