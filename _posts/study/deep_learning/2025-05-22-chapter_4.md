---
title: "Deep learning Chapter 4 - Single layer Networks: Regression "
date: 2025-05-24 18:00:00 +0900
categories:
  - Deep Learning
tags:
  - Machine Learning
description: 
toc: true
comments: false
cdn: 
image:
math: true
pin: false
mermaid: false
---

본 글은 **"Deep Learning: Foundations and Concepts"** (저자: Christopher Bishop)의 4장, Single layer Networks: Regression 내용을 한국어로 정리한 것입니다.

## 4.1 Linear Regression

- 입력 벡터: $$\mathbf{x} = (x_1, \dots, x_D)^T$$
- 목표값: $$t \in \mathbb{R}$$
- 선형 모델:
  $$
  y(\mathbf{x}, \mathbf{w}) = w_0 + w_1 x_1 + \dots + w_D x_D = \mathbf{w}^T \mathbf{x}
  $$

---

## 4.1.1 Basis Functions

- 선형 모델의 확장 → 비선형 변환 $\phi_j(x)$ 사용:
  $$
  y(\mathbf{x}, \mathbf{w}) = \sum_{j=0}^{M-1} w_j \phi_j(\mathbf{x}) = \mathbf{w}^T \boldsymbol{\phi}(\mathbf{x})
  $$

- 예시 기저 함수들:
  - 다항식: $$\phi_j(x) = x^j$$
  - 가우시안: 
    $$
    \phi_j(x) = \exp\left( -\frac{(x - \mu_j)^2}{2s^2} \right)
    $$
  - 시그모이드:
    $$
    \phi_j(x) = \sigma\left( \frac{x - \mu_j}{s} \right), \quad \sigma(a) = \frac{1}{1 + e^{-a}}
    $$
  - tanh 함수:
    $$
    \tanh(a) = 2\sigma(2a) - 1
    $$

---

## 4.1.2 Likelihood Function

- 모델: 
  $$
  t = y(\mathbf{x}, \mathbf{w}) + \varepsilon, \quad \varepsilon \sim \mathcal{N}(0, \sigma^2)
  $$

- 조건부 확률:
  $$
  p(t \mid \mathbf{x}, \mathbf{w}, \sigma^2) = \mathcal{N}(t \mid y(\mathbf{x}, \mathbf{w}), \sigma^2)
  $$

- 전체 데이터에 대한 우도:
  $$
  p(\mathbf{t} \mid X, \mathbf{w}, \sigma^2) = \prod_{n=1}^N \mathcal{N}(t_n \mid \mathbf{w}^T \boldsymbol{\phi}(\mathbf{x}_n), \sigma^2)
  $$

- 로그 우도:
  $$
  \log p = -\frac{N}{2} \log \sigma^2 - \frac{N}{2} \log(2\pi) - \frac{1}{\sigma^2} E_D(\mathbf{w})
  $$

- 오차 함수:
  $$
  E_D(\mathbf{w}) = \frac{1}{2} \sum_{n=1}^N \left( t_n - \mathbf{w}^T \boldsymbol{\phi}(\mathbf{x}_n) \right)^2
  $$

---

## 4.1.3 Maximum Likelihood

- 최대우도 해:
  $$
  \mathbf{w}_{ML} = \left( \Phi^T \Phi \right)^{-1} \Phi^T \mathbf{t}
  $$

- 분산 추정:
  $$
  \sigma^2_{ML} = \frac{1}{N} \sum_{n=1}^N \left( t_n - \mathbf{w}_{ML}^T \boldsymbol{\phi}(\mathbf{x}_n) \right)^2
  $$

---

## 4.1.4 Geometry of Least Squares

- 직교 투영 해:
  $$
  \mathbf{y} = \Phi \mathbf{w}_{ML}, \quad \mathbf{P} = \Phi \left( \Phi^T \Phi \right)^{-1} \Phi^T
  $$

- $\mathbf{P}$는 $\mathbf{t}$를 부분공간 $S$로 투영

---

## 4.1.5 Sequential Learning

- 확률적 경사 하강법 (SGD):
  $$
  \mathbf{w}^{(\tau+1)} = \mathbf{w}^{(\tau)} + \eta \left( t_n - \mathbf{w}^{(\tau)T} \phi_n \right) \phi_n
  $$

- LMS (Least-Mean-Squares) 알고리즘

---

## 4.1.6 Regularized Least Squares

- 정규화된 오차 함수:
  $$
  E(\mathbf{w}) = \frac{1}{2} \sum_{n=1}^N \left( t_n - \mathbf{w}^T \boldsymbol{\phi}(\mathbf{x}_n) \right)^2 + \frac{\lambda}{2} \mathbf{w}^T \mathbf{w}
  $$

- 해:
  $$
  \mathbf{w} = \left( \lambda I + \Phi^T \Phi \right)^{-1} \Phi^T \mathbf{t}
  $$

---

## 4.1.7 Multiple Outputs

- 출력 벡터: $$\mathbf{t} = (t_1, \dots, t_K)^T$$
- 모델:
  $$
  \mathbf{y}(x, W) = W^T \boldsymbol{\phi}(x)
  $$

- 최대우도 해:
  $$
  W_{ML} = \left( \Phi^T \Phi \right)^{-1} \Phi^T T
  $$

---

## 4.2 Decision Theory

- 예측 분포:
  $$
  p(t \mid x) = \mathcal{N}(t \mid y(x, \mathbf{w}_{ML}), \sigma^2_{ML})
  $$

- 기대 손실:
  $$
  \mathbb{E}[L] = \iint L(t, f(x)) p(x, t) \, dx \, dt
  $$

- 제곱 손실을 사용할 때:
  $$
  f^*(x) = \mathbb{E}[t \mid x]
  $$

- 분해:
  $$
  \mathbb{E}[L] = \int (f(x) - \mathbb{E}[t \mid x])^2 p(x) dx + \int \text{Var}[t \mid x] p(x) dx
  $$

- 일반적인 $L_q$ 손실:
  $$
  \mathbb{E}[L_q] = \iint |f(x) - t|^q p(x, t) dx dt
  $$

---

## 4.3 Bias–Variance Trade-off

- 기대 오차 분해:
  $$
  \mathbb{E}[L] = \text{Bias}^2 + \text{Variance} + \text{Noise}
  $$

- Bias:
  $$
  \text{Bias}^2 = \int \left( \mathbb{E}_{\mathcal{D}}[f(x; \mathcal{D})] - h(x) \right)^2 p(x) dx
  $$

- Variance:
  $$
  \text{Var} = \int \mathbb{E}_{\mathcal{D}} \left[ \left( f(x; \mathcal{D}) - \mathbb{E}_{\mathcal{D}}[f(x; \mathcal{D})] \right)^2 \right] p(x) dx
  $$

- Noise:
  $$
  \text{Noise} = \iint (h(x) - t)^2 p(x, t) dx dt
  $$

- 모델 복잡도 증가 시:
  $$
  \text{Bias} \downarrow, \quad \text{Variance} \uparrow
  $$

- 최적 정규화:
  $$
  \lambda^* = \arg \min_\lambda \left( \text{Bias}^2 + \text{Variance} \right)
  $$
