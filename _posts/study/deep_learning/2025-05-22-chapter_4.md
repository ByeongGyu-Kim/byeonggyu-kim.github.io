---
title: "Deep learning Chapter 4 - Single layer Networks: Regression "
date: 2025-05-22 18:00:00 +0900
categories:
  - Deep Learning
tags:
  - Machine Learning
description: 
toc: true
comments: false
cdn: 
image:
math: true
pin: false
mermaid: false
---

ë³¸ ê¸€ì€ **"Deep Learning: Foundations and Concepts"** (ì €ì: Christopher Bishop)ì˜ 4ì¥, Single layer Networks: Regression ë‚´ìš©ì„ í•œêµ­ì–´ë¡œ ì •ë¦¬í•œ ê²ƒì…ë‹ˆë‹¤.

## 1. Linear Regression
ì„ í˜• íšŒê·€ëŠ” ì—°ì†ì ì¸ ëª©í‘œê°’ $$t$$ë¥¼ ì˜ˆì¸¡í•˜ê¸° ìœ„í•´ ì…ë ¥ ë²¡í„° $$\mathbf{x}$$ì— ê¸°ë°˜í•˜ì—¬ ëª¨ë¸ì„ êµ¬ì„±í•˜ê²Œ ë˜ë©° ê°€ì¥ ë‹¨ìˆœí•œ í˜•íƒœë¥¼ ìƒê°í•´ë³´ê²Œ ë˜ë©´ ì„ í˜• ê²°í•©ì„ ì´ìš©í•˜ëŠ” ë°©ë²•ì´ë‹¤.

ì…ë ¥ì´ $$\mathbf{x} = (x_1, \dots, x_D)^T$$ì¼ ë•Œ, ì„ í˜• íšŒê·€ ëª¨ë¸ì€ ë‹¤ìŒê³¼ ê°™ì´ ë‚˜íƒ€ë‚¼ ìˆ˜ ìˆìœ¼ë©°, ì´ëŠ” íŒŒë¼ë¯¸í„° $$\mathbf{w}$$ì— ëŒ€í•´ ì„ í˜•ì…ë‹ˆë‹¤. í•˜ì§€ë§Œ ì…ë ¥ $$\mathbf{x}$$ì—ë„ ì„ í˜•ì´ê¸° ë•Œë¬¸ì—, ë³µì¡í•œ íŒ¨í„´ì„ í¬ì°©í•˜ëŠ” ë° í•œê³„ê°€ ìˆìŠµë‹ˆë‹¤.
$$
y(\mathbf{x}, \mathbf{w}) = w_0 + w_1 x_1 + \dots + w_D x_D = \mathbf{w}^T \mathbf{x}
$$

## ğŸ”¸ Basis Function í™•ì¥

ì„ í˜• ëª¨ë¸ì˜ í‘œí˜„ë ¥ì„ ë†’ì´ê¸° ìœ„í•´, ì…ë ¥ì— ë¹„ì„ í˜• ë³€í™˜ì„ ì ìš©í•œ basis functionì„ ì‚¬ìš©í•©ë‹ˆë‹¤. ì´ë¥¼ í†µí•´ ë¹„ì„ í˜• ê´€ê³„ë„ í¬ì°©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

$$
y(\mathbf{x}, \mathbf{w}) = \sum_{j=0}^{M-1} w_j \phi_j(\mathbf{x}) = \mathbf{w}^T \boldsymbol{\phi}(\mathbf{x})
$$

ì—¬ê¸°ì„œ $$\phi_0(\mathbf{x}) = 1$$ë¡œ ì •ì˜í•˜ë©´ $$w_0$$ëŠ” biasë¡œ í•´ì„ë©ë‹ˆë‹¤.


## 4.1.1 Basis Functions

- ì„ í˜• ëª¨ë¸ì˜ í™•ì¥ â†’ ë¹„ì„ í˜• ë³€í™˜ $\phi_j(x)$ ì‚¬ìš©:
  $$
  y(\mathbf{x}, \mathbf{w}) = \sum_{j=0}^{M-1} w_j \phi_j(\mathbf{x}) = \mathbf{w}^T \boldsymbol{\phi}(\mathbf{x})
  $$

- ì˜ˆì‹œ ê¸°ì € í•¨ìˆ˜ë“¤:
  - ë‹¤í•­ì‹: $$\phi_j(x) = x^j$$
  - ê°€ìš°ì‹œì•ˆ: 
    $$
    \phi_j(x) = \exp\left( -\frac{(x - \mu_j)^2}{2s^2} \right)
    $$
  - ì‹œê·¸ëª¨ì´ë“œ:
    $$
    \phi_j(x) = \sigma\left( \frac{x - \mu_j}{s} \right), \quad \sigma(a) = \frac{1}{1 + e^{-a}}
    $$
  - tanh í•¨ìˆ˜:
    $$
    \tanh(a) = 2\sigma(2a) - 1
    $$

---

## 4.1.2 Likelihood Function

- ëª¨ë¸: 
  $$
  t = y(\mathbf{x}, \mathbf{w}) + \varepsilon, \quad \varepsilon \sim \mathcal{N}(0, \sigma^2)
  $$

- ì¡°ê±´ë¶€ í™•ë¥ :
  $$
  p(t \mid \mathbf{x}, \mathbf{w}, \sigma^2) = \mathcal{N}(t \mid y(\mathbf{x}, \mathbf{w}), \sigma^2)
  $$

- ì „ì²´ ë°ì´í„°ì— ëŒ€í•œ ìš°ë„:
  $$
  p(\mathbf{t} \mid X, \mathbf{w}, \sigma^2) = \prod_{n=1}^N \mathcal{N}(t_n \mid \mathbf{w}^T \boldsymbol{\phi}(\mathbf{x}_n), \sigma^2)
  $$

- ë¡œê·¸ ìš°ë„:
  $$
  \log p = -\frac{N}{2} \log \sigma^2 - \frac{N}{2} \log(2\pi) - \frac{1}{\sigma^2} E_D(\mathbf{w})
  $$

- ì˜¤ì°¨ í•¨ìˆ˜:
  $$
  E_D(\mathbf{w}) = \frac{1}{2} \sum_{n=1}^N \left( t_n - \mathbf{w}^T \boldsymbol{\phi}(\mathbf{x}_n) \right)^2
  $$

---

## 4.1.3 Maximum Likelihood

- ìµœëŒ€ìš°ë„ í•´:
  $$
  \mathbf{w}_{ML} = \left( \Phi^T \Phi \right)^{-1} \Phi^T \mathbf{t}
  $$

- ë¶„ì‚° ì¶”ì •:
  $$
  \sigma^2_{ML} = \frac{1}{N} \sum_{n=1}^N \left( t_n - \mathbf{w}_{ML}^T \boldsymbol{\phi}(\mathbf{x}_n) \right)^2
  $$

---

## 4.1.4 Geometry of Least Squares

- ì§êµ íˆ¬ì˜ í•´:
  $$
  \mathbf{y} = \Phi \mathbf{w}_{ML}, \quad \mathbf{P} = \Phi \left( \Phi^T \Phi \right)^{-1} \Phi^T
  $$

- $\mathbf{P}$ëŠ” $\mathbf{t}$ë¥¼ ë¶€ë¶„ê³µê°„ $S$ë¡œ íˆ¬ì˜

---

## 4.1.5 Sequential Learning

- í™•ë¥ ì  ê²½ì‚¬ í•˜ê°•ë²• (SGD):
  $$
  \mathbf{w}^{(\tau+1)} = \mathbf{w}^{(\tau)} + \eta \left( t_n - \mathbf{w}^{(\tau)T} \phi_n \right) \phi_n
  $$

- LMS (Least-Mean-Squares) ì•Œê³ ë¦¬ì¦˜

---

## 4.1.6 Regularized Least Squares

- ì •ê·œí™”ëœ ì˜¤ì°¨ í•¨ìˆ˜:
  $$
  E(\mathbf{w}) = \frac{1}{2} \sum_{n=1}^N \left( t_n - \mathbf{w}^T \boldsymbol{\phi}(\mathbf{x}_n) \right)^2 + \frac{\lambda}{2} \mathbf{w}^T \mathbf{w}
  $$

- í•´:
  $$
  \mathbf{w} = \left( \lambda I + \Phi^T \Phi \right)^{-1} \Phi^T \mathbf{t}
  $$

---

## 4.1.7 Multiple Outputs

- ì¶œë ¥ ë²¡í„°: $$\mathbf{t} = (t_1, \dots, t_K)^T$$
- ëª¨ë¸:
  $$
  \mathbf{y}(x, W) = W^T \boldsymbol{\phi}(x)
  $$

- ìµœëŒ€ìš°ë„ í•´:
  $$
  W_{ML} = \left( \Phi^T \Phi \right)^{-1} \Phi^T T
  $$

---

## 4.2 Decision Theory

- ì˜ˆì¸¡ ë¶„í¬:
  $$
  p(t \mid x) = \mathcal{N}(t \mid y(x, \mathbf{w}_{ML}), \sigma^2_{ML})
  $$

- ê¸°ëŒ€ ì†ì‹¤:
  $$
  \mathbb{E}[L] = \iint L(t, f(x)) p(x, t) \, dx \, dt
  $$

- ì œê³± ì†ì‹¤ì„ ì‚¬ìš©í•  ë•Œ:
  $$
  f^*(x) = \mathbb{E}[t \mid x]
  $$

- ë¶„í•´:
  $$
  \mathbb{E}[L] = \int (f(x) - \mathbb{E}[t \mid x])^2 p(x) dx + \int \text{Var}[t \mid x] p(x) dx
  $$

- ì¼ë°˜ì ì¸ $L_q$ ì†ì‹¤:
  $$
  \mathbb{E}[L_q] = \iint |f(x) - t|^q p(x, t) dx dt
  $$

---

## 4.3 Biasâ€“Variance Trade-off

- ê¸°ëŒ€ ì˜¤ì°¨ ë¶„í•´:
  $$
  \mathbb{E}[L] = \text{Bias}^2 + \text{Variance} + \text{Noise}
  $$

- Bias:
  $$
  \text{Bias}^2 = \int \left( \mathbb{E}_{\mathcal{D}}[f(x; \mathcal{D})] - h(x) \right)^2 p(x) dx
  $$

- Variance:
  $$
  \text{Var} = \int \mathbb{E}_{\mathcal{D}} \left[ \left( f(x; \mathcal{D}) - \mathbb{E}_{\mathcal{D}}[f(x; \mathcal{D})] \right)^2 \right] p(x) dx
  $$

- Noise:
  $$
  \text{Noise} = \iint (h(x) - t)^2 p(x, t) dx dt
  $$

- ëª¨ë¸ ë³µì¡ë„ ì¦ê°€ ì‹œ:
  $$
  \text{Bias} \downarrow, \quad \text{Variance} \uparrow
  $$

- ìµœì  ì •ê·œí™”:
  $$
  \lambda^* = \arg \min_\lambda \left( \text{Bias}^2 + \text{Variance} \right)
  $$
