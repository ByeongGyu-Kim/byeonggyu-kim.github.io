---
title: "On Calibration of Modern Neural Networks"
date: 2025-05-19 18:00:00 +0900
categories:
  - Paper-Review
  - Neural Networks
tags:
  - Machine Learning
description: 
toc: true
comments: false
cdn: 
image:
math: true
pin: false
mermaid: false
---

## ğŸ“Œ ì—°êµ¬ ê°œìš”
**Confidence Calibration**ì€ ëª¨ë¸ì´ ì˜ˆì¸¡í•œ í™•ë¥ ì´ ì‹¤ì œ ì •ë‹µì¼ ê°€ëŠ¥ì„±ê³¼ ì–¼ë§ˆë‚˜ ì¼ì¹˜í•˜ëŠ”ì§€ë¥¼ ë‚˜íƒ€ë‚´ëŠ” ê°œë…ì´ë‹¤. ì˜ˆë¥¼ ë“¤ì–´, ì–´ë–¤ ì´ë¯¸ì§€ì— ëŒ€í•´ ëª¨ë¸ì´ ê³ ì–‘ì´ì¼ í™•ë¥ ì„ 0.9ë¼ê³  ì˜ˆì¸¡í–ˆì„ ë•Œ, ì´ëŸ¬í•œ ì˜ˆì¸¡ì´ ì˜ ë³´ì •(calibrated)ë˜ì–´ ìˆë‹¤ë©´ ì‹¤ì œë¡œ ê·¸ ì´ë¯¸ì§€ê°€ ê³ ì–‘ì´ì¼ í™•ë¥ ë„ ì•½ 90%ê°€ ë˜ì–´ì•¼ í•œë‹¤ëŠ” ê²ƒì´ë‹¤.

**On Calibration of Modern Neural Networks** ë…¼ë¬¸ì€ ë¹ ë¥´ê²Œ ë°œì „í•˜ê³  ì—°êµ¬ë˜ì–´ì ¸ ì˜¤ê³  ìˆëŠ” ResNet, DenseNet ë“±ê³¼ ê°™ì€ í˜„ëŒ€ì ì¸ ì‹ ê²½ë§ ëª¨ë¸ë“¤ì´ ë†’ì€ ë¶„ë¥˜ ì •í™•ë„ë¥¼ ë‹¬ì„±í•¨ì—ë„ ë¶ˆêµ¬í•˜ê³ , ì˜¤íˆë ¤ í™•ë¥  ë³´ì •(calibration) ì„±ëŠ¥ì€ ë” ë‚˜ë¹ ì¡Œë‹¤ëŠ” ì‚¬ì‹¤ì„ ì‹¤í—˜ì ìœ¼ë¡œ ë³´ì´ê³  ìˆë‹¤. ê³¼ê±°ì˜ ì–•ì€ ì‹ ê²½ë§ ëª¨ë¸ë“¤ì€ ì˜ˆì¸¡ í™•ë¥ ì´ ì‹¤ì œ ì •ë‹µ í™•ë¥ ê³¼ ë¹„êµì  ì˜ ì¼ì¹˜í–ˆì§€ë§Œ, ê¹Šê³  ë³µì¡í•œ êµ¬ì¡°ë¥¼ ê°€ì§„ ìµœì‹  ëª¨ë¸ë“¤ì€ ìì‹  ìˆê²Œ ì˜ˆì¸¡ì„ í•˜ë©´ì„œë„ ê·¸ í™•ë¥ ì´ ì‹¤ì œ ì •ë‹µë¥ ê³¼ ë¶ˆì¼ì¹˜í•˜ëŠ” ê²½í–¥ì´ ìˆë‹¤ëŠ” ì ì„ ì§šê³  ìˆë‹¤. í•œ ë§ˆë””ë¡œ ë§í•´ ìµœì‹  ëª¨ë¸ë“¤ì€ Overconfident ë˜ì–´ì ¸ ìˆë‹¤ëŠ” ì ì„ ë°œê²¬í•˜ì˜€ë‹¤.


![Desktop View](/assets/img/paper-review/On_Calibration_of_modern_NN/figure1.png)
_Figure 1.1: Confidence histograms (top) and reliability diagrams (bottom) for a 5-layer LeNet (left) and a 110-layer ResNet (right) on CIFAR-100. Refer to the text below for detailed illustration._


ì´ëŸ¬í•œ ë¬¸ì œëŠ” ììœ¨ì£¼í–‰, ì˜ë£Œ ì§„ë‹¨, ë²•ë¥  íŒë‹¨ ë“±ê³¼ ê°™ì´ ì˜ˆì¸¡ ê²°ê³¼ì— ëŒ€í•œ ì‹ ë¢°ë„ê°€ ë§¤ìš° ì¤‘ìš”í•œ ì‘ìš© ë¶„ì•¼ì—ì„œ íŠ¹íˆ ë„ë“œë¼ì§ˆ ìˆ˜ ìˆê¸°ì— ëª¨ë¸ì´ ë‹¨ìˆœíˆ ì •í™•í•  ë¿ë§Œ ì•„ë‹ˆë¼, ìì‹ ì˜ ì˜ˆì¸¡ì´ ì–¼ë§ˆë‚˜ í™•ì‹¤í•œì§€ì— ëŒ€í•œ í‘œí˜„ ë˜í•œ ì‹ ë¢°í•  ìˆ˜ ìˆì–´ì•¼ í•œë‹¤.

ë³¸ ë…¼ë¬¸ì—ì„œëŠ” ì´ëŸ¬í•œ ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ ë‹¤ì–‘í•œ ì‚¬í›„ ë³´ì •(post-hoc calibration) ë°©ë²•ë“¤ì„ ì‹¤í—˜ì ìœ¼ë¡œ ë¹„êµí•˜ê³ , ê·¸ ì¤‘ì—ì„œë„ Temperature Scalingì´ë¼ëŠ” ë‹¨ í•˜ë‚˜ì˜ ìŠ¤ì¹¼ë¼ íŒŒë¼ë¯¸í„°ë§Œì„ ì‚¬ìš©í•˜ëŠ” ê°„ë‹¨í•œ ë°©ë²•ì´ ë§¤ìš° íš¨ê³¼ì ì´ë¼ëŠ” ì‚¬ì‹¤ì„ ë°í˜€ëƒˆë‹¤. ë³¸ ê¸€ì—ì„œëŠ” ì´ì˜ ì‹¤í—˜ ì½”ë“œë„ êµ¬í˜„í•˜ì˜€ë‹¤.

## ğŸ” ì‹ ê²½ë§ì˜ Overconfidence ì›ì¸ ë¶„ì„

ìµœê·¼ì˜ ì‹ ê²½ë§ ëª¨ë¸ë“¤ì€ ë†’ì€ ì •í™•ë„ë¥¼ ìë‘í•˜ì§€ë§Œ, ê·¸ **confidence (ì˜ˆì¸¡ í™•ë¥ )** ëŠ” ì‹¤ì œ ì •ë‹µë¥ ê³¼ ì˜ ë§ì§€ ì•ŠëŠ” ê²½ìš°ê°€ ë§ë‹¤. ì´ í˜„ìƒì„ **miscalibration (ë¶ˆì™„ì „í•œ ë³´ì •)** ì´ë¼ê³  í•˜ë©°, ê·¸ ì›ì¸ê³¼ ê´€ë ¨ ìš”ì†Œë“¤ì„ ìš°ì„  ë¶„ì„í•˜ì˜€ë‹¤.


![Desktop View](/assets/img/paper-review/On_Calibration_of_modern_NN/figure2.png)
_Figure 2: The effect of network depth (far left), width (middle left), Batch Normalization (middle right), and weight decay (far right) on
miscalibration, as measured by ECE (lower is better)_


### 1. ëª¨ë¸ ìš©ëŸ‰ì˜ ì¦ê°€ (Model Capacity)

- ìµœê·¼ ë”¥ëŸ¬ë‹ ëª¨ë¸ë“¤ì€ ë ˆì´ì–´ ìˆ˜ì™€ í•„í„° ìˆ˜ê°€ ê¸‰ê²©íˆ ì¦ê°€í•˜ì—¬, í•™ìŠµ ë°ì´í„°ë¥¼ ë” ì˜ ë§ì¶œ ìˆ˜ ìˆëŠ” **ëª¨ë¸ ìš©ëŸ‰(capacity)** ì„ ê°–ì¶”ê²Œ ë˜ì—ˆë‹¤.
- í•˜ì§€ë§Œ ëª¨ë¸ ìš©ëŸ‰ì´ ì»¤ì§ˆìˆ˜ë¡ ì˜¤íˆë ¤ **confidenceê°€ ì‹¤ì œ ì •í™•ë„ë³´ë‹¤ ê³¼ë„í•˜ê²Œ ë†’ì•„ì§€ëŠ” ê³¼ì‹ , ì¦‰ overconfidence** í•˜ëŠ” ê²½í–¥ì´ ë‚˜íƒ€ë‚œë‹¤.

ì‹¤í—˜ ê²°ê³¼ (ResNet on CIFAR-100):
- ê¹Šì´(depth)ë¥¼ ì¦ê°€ì‹œí‚¤ë©´ Errorì€ ì¤„ì–´ë“œë‚˜ ECEê°€ ì¦ê°€
- í•„í„° ìˆ˜(width)ë¥¼ ì¦ê°€ì‹œí‚¤ë©´ Errorì€ í™•ì—°íˆ ì¤„ì–´ë“œë‚˜, ECEê°€ ì¦ê°€

> ë†’ì€ capacityëŠ” overfittingì„ ì•¼ê¸°í•  ìˆ˜ ìˆìœ¼ë©°, ì´ëŸ¬í•œ ê²½ìš° ì •í™•ë„ëŠ” ì¢‹ì•„ì ¸ë„ í™•ë¥ ì˜ í’ˆì§ˆì€ ë‚˜ë¹ ì§„ë‹¤.
{: .prompt-tip }

---

### 2. Batch Normalizationì˜ ì˜í–¥

- **Batch Normalization**ì€ ë”¥ëŸ¬ë‹ ëª¨ë¸ì˜ í•™ìŠµì„ ì•ˆì •í™”ì‹œí‚¤ê³  ë¹ ë¥´ê²Œ ë§Œë“œëŠ” ê¸°ë²•ìœ¼ë¡œ, í˜„ëŒ€ ì•„í‚¤í…ì²˜ì—ì„œ í•„ìˆ˜ì ìœ¼ë¡œ ì‚¬ìš©ëœë‹¤.
- í•˜ì§€ë§Œ, BNì„ ì‚¬ìš©í•œ ëª¨ë¸ë“¤ì€ **ì •í™•ë„ëŠ” ì˜¬ë¼ê°€ì§€ë§Œ calibrationì€ ì˜¤íˆë ¤ ë‚˜ë¹ ì§€ëŠ”** í˜„ìƒì´ ë‚˜íƒ€ë‚œë‹¤.

ì‹¤í—˜ ê²°ê³¼ (6-layer ConvNet on CIFAR-100):
- BNì„ ì ìš©í•œ ConvNetì€ ì •í™•ë„ê°€ ì•½ê°„ ì˜¬ë¼ê°€ì§€ë§Œ(Error ê°ì†Œ), ECEëŠ” ëšœë ·í•˜ê²Œ ì¦ê°€

> BNì€ ë‚´ë¶€ í™œì„± ë¶„í¬ë¥¼ ì •ê·œí™”í•˜ì—¬ í•™ìŠµì„ ë” ì˜ ë˜ê²Œ í•˜ì§€ë§Œ, ê²°ê³¼ì ìœ¼ë¡œ ì¶œë ¥ í™•ë¥ ì´ ë” ê³¼ì‹ ëœ(overconfident) ìƒíƒœë¡œ ë‚˜íƒ€ë‚˜ calibrationì—ëŠ” ë¶€ì •ì ì¸ ì˜í–¥ì„ ë¯¸ì¹˜ê²Œ ëœë‹¤.
{: .prompt-tip }
---

### 3. Weight Decay ê°ì†Œì˜ ì˜í–¥

- ì „í†µì ìœ¼ë¡œ **weight decay** ëŠ” ê³¼ì í•©ì„ ë§‰ê¸° ìœ„í•œ ì •ê·œí™” ë°©ë²•ìœ¼ë¡œ ë„ë¦¬ ì‚¬ìš©ë˜ì–´ ì™”ìœ¼ë©°, overfittingì„ ë°©ì§€í•˜ê¸° ìœ„í•´ ê°€ì¤‘ì¹˜ì— íŒ¨ë„í‹°ë¥¼ ì£¼ëŠ” ì •ê·œí™” ê¸°ë²•ì´ë‹¤.
- ìµœê·¼ì—ëŠ” BNì˜ ì •ê·œí™” íš¨ê³¼ ë•Œë¬¸ì— weight decay ì‚¬ìš©ëŸ‰ì´ ì¤„ì–´ë“œëŠ” ì¶”ì„¸ì´ë‹¤.
- í•˜ì§€ë§Œ ì‹¤í—˜ì—ì„œëŠ” **weight decayë¥¼ ì¦ê°€ì‹œí‚¬ìˆ˜ë¡ calibrationì€ ê°œì„ ë˜ëŠ” ê²½í–¥**ì„ ë³´ì¸ë‹¤.

ì‹¤í—˜ ê²°ê³¼ (ResNet-110 on CIFAR-100):
- Weight decayë¥¼ ì¦ê°€ì‹œí‚¤ë©´ ë¶„ë¥˜ ì •í™•ë„(Error)ëŠ” íŠ¹ì • êµ¬ê°„ì—ì„œ ìµœì ì ì„ ì°ê³  ì´í›„ ë‹¤ì‹œ ì¦ê°€
- ECEëŠ” weight decayê°€ ì¦ê°€í• ìˆ˜ë¡ ê°ì†Œí•˜ëŠ” ê²½í–¥ì„ ë³´ì„

> ì¦‰, ì •í™•ë„ë¥¼ ìµœëŒ€í™”í•˜ëŠ” weight decay ì„¤ì •ê³¼ calibrationì„ ìµœì í™”í•˜ëŠ” ì„¤ì •ì€ ì„œë¡œ ë‹¤ë¥¼ ìˆ˜ ìˆìœ¼ë©°, ì •í™•ë„ëŠ” ìœ ì§€ë˜ë”ë¼ë„ confidenceëŠ” ì™œê³¡ë  ìˆ˜ ìˆë‹¤.
{: .prompt-tip }
---

### 4. NLL ê³¼ì í•© í˜„ìƒ (Overfitting to NLL)

![Desktop View](/assets/img/paper-review/On_Calibration_of_modern_NN/figure3.png)
_Figure 3: Test error and NLL of a 110-layer ResNet with stochastic depth on CIFAR-100 during training_

- ì‹¤í—˜ì—ì„œëŠ” learning rateê°€ ë‚®ì•„ì§€ëŠ” êµ¬ê°„ì—ì„œ test errorëŠ” ê³„ì† ì¤„ì–´ë“œëŠ” ë°˜ë©´, NLLì€ ë‹¤ì‹œ ì¦ê°€í•˜ëŠ” í˜„ìƒì„ í™•ì¸í•˜ì˜€ë‹¤.
- ì´ëŠ” ëª¨ë¸ì´ **ì •í™•ë„ëŠ” ë†’ì´ì§€ë§Œ confidenceê°€ ì‹¤ì œë³´ë‹¤ ê³¼ë„í•œ ìƒíƒœë¡œ í•™ìŠµì´ ì§„í–‰ë˜ê³  ìˆìŒ**ì„ ì˜ë¯¸í•œë‹¤.

ì‹¤í—˜ ê²°ê³¼ (ResNet-110 + stochastic depth on CIFAR-100):
- Epoch 250 ì´í›„ learning rate ê°ì†Œ
- ì´í›„ test errorëŠ” ê°ì†Œ (29% â†’ 27%)í•˜ì§€ë§Œ, NLLì€ ì¦ê°€

> ìµœì‹  ì‹ ê²½ë§ì€ í•™ìŠµ í›„ë°˜ë¶€ì—ì„œ NLLì„ ê³„ì† ìµœì†Œí™”í•˜ë ¤ëŠ” ê³¼ì •ì—ì„œ confidenceë¥¼ ê³¼ë„í•˜ê²Œ ë†’ì´ëŠ” ê²½í–¥ì´ ìˆìœ¼ë©°, ì´ë¡œ ì¸í•´ ì‹¤ì œ ì •ë‹µë¥ ë³´ë‹¤ ë†’ì€ í™•ë¥ ì„ ì¶œë ¥í•˜ëŠ” overconfidentí•œ ìƒíƒœë¡œ calibration ì˜¤ë¥˜ê°€ ë°œìƒí•œë‹¤.
{: .prompt-tip }


## ğŸ“ Calibrationì˜ ì •ì˜ ë° ì¸¡ì • ë°©ë²•

ë³¸ ë…¼ë¬¸ì—ì„œëŠ” ë‹¤ì¤‘ í´ë˜ìŠ¤ ë¶„ë¥˜ ë¬¸ì œë¥¼ ë‹¤ë£¨ê³  ìˆìœ¼ë©°, ë”¥ëŸ¬ë‹ ëª¨ë¸ì€ ì£¼ì–´ì§„ ì…ë ¥ $$X \in \mathcal{X}, \quad Y \in \{1, \dots, K\}$$ ë¥¼ ì˜ˆì¸¡í•˜ëŠ”ë‹¤ê³  ê°€ì •í•œë‹¤. ëª¨ë¸ì˜ ì˜ˆì¸¡ í™•ë¥ ì€ ë‹¤ìŒê³¼ ê°™ì´ ì •ì˜ëœë‹¤.

$$
h(X) = (\hat{Y}, \hat{P})
$$

ì—¬ê¸°ì„œ $$\hat{Y}$$ëŠ” ì˜ˆì¸¡ëœ í´ë˜ìŠ¤, $$\hat{P}$$ëŠ” ê° í´ë˜ìŠ¤ì— ëŒ€í•œ í™•ë¥  ë¶„í¬ì´ë©°, softmax ì¶œë ¥ì˜ ìµœëŒ“ê°’ìœ¼ë¡œ ì •ì˜ëœë‹¤.

ê·¸ëŸ¼ ì™„ë²½í•˜ê²Œ ë³´ì •ëœ ëª¨ë¸ì˜ ì •ì˜ëŠ” ì–´ë–»ê²Œ ë ê¹Œ? ë³¸ ë…¼ë¬¸ì—ì„œëŠ” ë‹¤ìŒê³¼ ê°™ì´ ì •ì˜í•˜ê³  ìˆë‹¤.

$$
P(\hat{Y} = Y \mid \hat{P} = p) = p, \quad \forall p \in [0, 1]
$$

ìœ„ ì‹ì—ì„œ ì•Œ ìˆ˜ ìˆë“¯ì´ ëª¨ë¸ì´ **ì™„ë²½íˆ ë³´ì •(calibrated)**ë˜ì–´ ìˆë‹¤ëŠ” ê²ƒì€, ì˜ˆì¸¡í•œ í™•ë¥  ê°’ì´ ì‹¤ì œ ì •ë‹µë¥ ê³¼ ì¼ì¹˜í•˜ëŠ” ê²ƒì„ ì˜ë¯¸í•œë‹¤. ì˜ˆë¥¼ ë“¤ì–´ ëª¨ë¸ì´ 100ê°œì˜ ìƒ˜í”Œì— ëŒ€í•´ ëª¨ë‘ 0.8ì˜ confidenceë¥¼ ì¶œë ¥í–ˆë‹¤ë©´, ì‹¤ì œë¡œ ê·¸ ì¤‘ ì•½ 80ê°œê°€ ë§ì•„ì•¼ ì™„ë²½íˆ ë³´ì •ëœ ê²ƒì´ë‹¤.

### ğŸ“Š ì‹¤ì „ì—ì„œëŠ” ì–´ë–»ê²Œ ì¸¡ì •í•˜ëŠ”ê°€?

#### ğŸ” Reliability Diagram (ì‹ ë¢°ë„ ë‹¤ì´ì–´ê·¸ë¨)

ì˜ˆì¸¡ í™•ë¥  $$\hat{P}$$ë¥¼ êµ¬ê°„ìœ¼ë¡œ ì˜ê²Œ ë‚˜ëˆ„ê³ , ê° êµ¬ê°„ì—ì„œì˜ **ì‹¤ì œ ì •ë‹µë¥ (accuracy)**ê³¼ **í‰ê·  confidence**ë¥¼ ë¹„êµí•œë‹¤. ë§Œì•½ ëª¨ë¸ì´ ì˜ ë³´ì •ë˜ì–´ì ¸ ìˆë‹¤ë©´, ê° êµ¬ê°„ì—ì„œëŠ” ì•„ë˜ì˜ ê´€ê³„ì‹ì´ ì„±ë¦½í•´ì•¼í•œë‹¤ëŠ” ê²ƒì´ë‹¤.

$$
\text{acc}(B_m) = \frac{1}{|B_m|} \sum_{i \in B_m} \mathbf{1}(\hat{y}_i = y_i)
$$

$$
\text{conf}(B_m) = \frac{1}{|B_m|} \sum_{i \in B_m} \hat{p}_i
$$

$$
\text{Accuracy}(B_m) \approx \text{Confidence}(B_m)
$$

ì—¬ê¸°ì„œ $$\text{acc}(B_m)$$ëŠ” êµ¬ê°„ $$B_m$$ì— ì†í•˜ëŠ” ìƒ˜í”Œë“¤ì˜ ì‹¤ì œ ì •ë‹µë¥ , $$\text{conf}(B_m)$$ëŠ” êµ¬ê°„ $$B_m$$ì— ì†í•˜ëŠ” ìƒ˜í”Œë“¤ì˜ í‰ê·  confidenceë¥¼ ì˜ë¯¸í•œë‹¤. ë§Œì•½ ëª¨ë¸ì´ ì˜ ë³´ì •ë˜ì–´ ìˆë‹¤ë©´, ë‘ ê°’ì€ ì„œë¡œ ë¹„ìŠ·í•´ì•¼ í•œë‹¤.

#### ğŸ“ Expected Calibration Error (ECE)
ECEëŠ” ëª¨ë¸ì˜ ì „ì²´ calibration ì„±ëŠ¥ì„ ìˆ˜ì¹˜ì ìœ¼ë¡œ ì¸¡ì •í•˜ëŠ” ëŒ€í‘œì ì¸ ì§€í‘œë¡œ, ê° binì— ëŒ€í•´ ì˜ˆì¸¡ í™•ë¥ ê³¼ ì‹¤ì œ ì •ë‹µë¥  ê°„ì˜ ì°¨ì´ë¥¼ í‰ê· í•˜ì—¬ ê³„ì‚°í•œë‹¤. $$M$$ê°œì˜ binìœ¼ë¡œ ë‚˜ëˆ„ê³ , ê° bin $$B_m$$ì— ëŒ€í•´ ë‹¤ìŒê³¼ ê°™ì´ ì •ì˜ëœë‹¤.

$$
\text{ECE} = \sum_{m=1}^{M} \frac{|B_m|}{n} \left| \text{acc}(B_m) - \text{conf}(B_m) \right|
$$

#### ğŸ“ Maximum Calibration Error (MCE)
MCEëŠ” ê°€ì¥ í° ì˜¤ì°¨ë¥¼ ë³´ì¸ binì˜ calibration gapì„ ì¸¡ì •í•˜ê²Œ ë˜ë©°, ì‰½ê²Œ ë§í•´ â€œìµœì•…ì˜ ë³´ì • ì‹¤íŒ¨â€ ì •ë„ë¥¼ ë‚˜íƒ€ë‚¸ë‹¤ê³  ìƒê°í•  ìˆ˜ ìˆë‹¤. ì•ˆì „ì´ ì¤‘ìš”í•œ ì‹œìŠ¤í…œì—ì„œ ë§¤ìš° ì¤‘ìš”í•œ ì§€í‘œë¡œ ì‚¬ìš©ë  ìˆ˜ ìˆë‹¤.

$$
\text{MCE} = \max_{m \in \{1, \dots, M\}} \left| \text{acc}(B_m) - \text{conf}(B_m) \right|
$$




## ğŸ› ï¸ Calibration Methods

ë³¸ ì¥ì—ì„œëŠ” ì´ë¯¸ í•™ìŠµëœ ëª¨ë¸ì— ëŒ€í•´ **í™•ë¥  ë³´ì •ì„ ìœ„í•œ ì‚¬í›„ ì²˜ë¦¬(Post-hoc) ë°©ë²•**ë“¤ì„ ì†Œê°œí•©ë‹ˆë‹¤. ì´ë“¤ì€ ëª¨ë¸ì˜ ì˜ˆì¸¡ êµ¬ì¡°ë‚˜ ì •í™•ë„ëŠ” ìœ ì§€í•˜ë©´ì„œ, **ì˜ˆì¸¡ í™•ë¥ (confidence)**ì´ ì‹¤ì œ ì •ë‹µë¥ ê³¼ ë” ì˜ ì¼ì¹˜í•˜ë„ë¡ ë§Œë“¤ì–´ì¤ë‹ˆë‹¤.

---

### ğŸ“˜ 4.1 ì´ì§„ ë¶„ë¥˜ì—ì„œì˜ Calibration

ëª¨ë¸ì€ ì…ë ¥ \( X \)ì— ëŒ€í•´ ë‹¤ìŒê³¼ ê°™ì€ ì˜ˆì¸¡ì„ ìˆ˜í–‰í•œë‹¤ê³  ê°€ì •í•©ë‹ˆë‹¤:

$$
h(X) = (\hat{Y}, \hat{P})
$$

ì—¬ê¸°ì„œ  
- \( \hat{Y} \): ì˜ˆì¸¡ëœ í´ë˜ìŠ¤  
- \( \hat{P} \): ì˜ˆì¸¡ í™•ë¥ , ì¼ë°˜ì ìœ¼ë¡œ softmax ë˜ëŠ” sigmoid ì¶œë ¥ì˜ ìµœëŒ€ê°’

---

#### ğŸ”¹ Histogram Binning

- ì˜ˆì¸¡ í™•ë¥  \( \hat{P} \in [0, 1] \) êµ¬ê°„ì„ ê· ë“±í•˜ê²Œ ë‚˜ëˆ„ê³ ,
- ê° êµ¬ê°„(bin)ë§ˆë‹¤ ì‹¤ì œ ì •ë‹µë¥ ì˜ í‰ê· ì„ ê³„ì‚°í•˜ì—¬ ë³´ì •ëœ í™•ë¥ ë¡œ ì‚¬ìš©

ì˜ˆ:
- 0.7â€“0.8 êµ¬ê°„ì— 100ê°œì˜ ìƒ˜í”Œì´ ìˆê³ , 75ê°œê°€ ì •ë‹µì´ë©´ â†’ ë³´ì • í™•ë¥ ì€ 0.75

---

#### ğŸ”¹ Isotonic Regression

- ë‹¨ì¡° ì¦ê°€ í•¨ìˆ˜ë¡œ í™•ë¥ ì„ ë³´ì •
- êµ¬ê°„ ê°„ ê³„ë‹¨í˜•ìœ¼ë¡œ ì¡°ì •ë˜ë©° ìœ ì—°í•˜ì§€ë§Œ ê³¼ì í•© ê°€ëŠ¥ì„± ì¡´ì¬

---

#### ğŸ”¹ Platt Scaling

- ë¡œì§“(logit)ì„ ì…ë ¥ìœ¼ë¡œ í•˜ëŠ” ë¡œì§€ìŠ¤í‹± íšŒê·€ ê¸°ë°˜ ë³´ì • ë°©ë²•

$$
\hat{q}_i = \sigma(a z_i + b)
$$

- íŒŒë¼ë¯¸í„° \( a, b \)ëŠ” validation setì—ì„œ í•™ìŠµ
- ê°„ë‹¨í•˜ê³  ì•ˆì •ì ì´ë©°, ì´ì§„ ë¶„ë¥˜ì— íš¨ê³¼ì 

---

### ğŸ“˜ 4.2 ë‹¤ì¤‘ í´ë˜ìŠ¤ ë¶„ë¥˜ì—ì„œì˜ í™•ì¥

ë‹¤ì¤‘ í´ë˜ìŠ¤ì—ì„œëŠ” softmaxë¥¼ ì‚¬ìš©í•˜ì—¬ ë‹¤ìŒê³¼ ê°™ì´ í™•ë¥ ì„ ê³„ì‚°í•©ë‹ˆë‹¤:

$$
\hat{P} = \max_k \left( \frac{e^{z_k}}{\sum_j e^{z_j}} \right)
$$

---

#### ğŸ”¹ Matrix Scaling

- ë¡œì§“ ë²¡í„° \( z \)ì— ì„ í˜• ë³€í™˜ ì ìš©:

$$
\hat{q} = \text{softmax}(W z + b)
$$

- \( W \in \mathbb{R}^{K \times K} \), \( b \in \mathbb{R}^K \)
- ê°•ë ¥í•œ í‘œí˜„ë ¥ì„ ê°€ì§€ì§€ë§Œ íŒŒë¼ë¯¸í„° ìˆ˜ê°€ ë§ì•„ ê³¼ì í•© ìš°ë ¤

---

#### ğŸ”¹ Vector Scaling

- Matrix Scalingì˜ ê°„ì†Œí™” ë²„ì „ìœ¼ë¡œ \( W \)ë¥¼ ëŒ€ê° í–‰ë ¬ë¡œ ì œí•œ

$$
\hat{q} = \text{softmax}(D z + b)
$$

- íŒŒë¼ë¯¸í„° ìˆ˜ê°€ \( 2K \)ê°œë¡œ ì¤„ì–´ë“¤ë©° ê³„ì‚°ëŸ‰ê³¼ ê³¼ì í•© ê°€ëŠ¥ì„±ì´ ê°ì†Œ

---

#### ğŸŒ¡ï¸ Temperature Scaling

- ê°€ì¥ ê°„ë‹¨í•˜ë©´ì„œë„ ê°•ë ¥í•œ ë³´ì • ê¸°ë²•
- ë¡œì§“ì„ ìŠ¤ì¹¼ë¼ \( T \)ë¡œ ë‚˜ëˆ„ê³  softmax ì ìš©

$$
\hat{q} = \text{softmax}(z / T)
$$

- \( T > 1 \): í™•ë¥ ì´ ë¶„ì‚°ë¨ (ê³¼ì‹  ì™„í™”)
- \( T = 1 \): ì›ë˜ ëª¨ë¸ê³¼ ë™ì¼
- \( T < 1 \): í™•ë¥ ì´ ë” sharpí•´ì§

---

### ğŸ“Œ Temperature Scalingì˜ íŠ¹ì§•

- ë‹¨ í•˜ë‚˜ì˜ íŒŒë¼ë¯¸í„° \( T \)ë§Œ ì¡°ì •
- ëª¨ë¸ì˜ ì˜ˆì¸¡ í´ë˜ìŠ¤ëŠ” ìœ ì§€ë˜ë©°, confidenceë§Œ ì¡°ì •ë¨
- ë‹¤ì¤‘ í´ë˜ìŠ¤ì—ì„œë„ ì†ì‰½ê²Œ ì ìš© ê°€ëŠ¥

---

### ğŸ§ª ì‹¤í—˜ ì¤‘ Temperature Scalingì˜ íš¨ê³¼

- Epoch 250 ì´í›„ learning rate ê°ì†Œ
- ì´í›„ test errorëŠ” ê°ì†Œ (29% â†’ 27%)í•˜ì§€ë§Œ, NLLì€ ì¦ê°€
- ì´ëŠ” ëª¨ë¸ì´ ë” ì •í™•í•´ì¡Œì§€ë§Œ, í™•ë¥  ë¶„í¬ëŠ” ë” ê³¼ì‹ í•˜ê²Œ ë˜ì—ˆìŒì„ ì˜ë¯¸
- Temperature Scalingì€ ì´ëŸ¬í•œ overconfidenceë¥¼ íš¨ê³¼ì ìœ¼ë¡œ ì™„í™”í•¨

---



## ğŸ§ª Python ì‹¤ìŠµ ê°œìš”
ì§€ê¸ˆë¶€í„°ëŠ” ì•ì„œ ì‚´í´ë³¸ ë…¼ë¬¸ê³¼ ê´€ë ¨í•˜ì—¬ ì‹¤ìŠµì„ ì§„í–‰í•´ë³´ê³ ì í•œë‹¤. ë³¸ ì‹¤í—˜ì€ CIFAR-100 ì´ë¯¸ì§€ ë¶„ë¥˜ ê³¼ì œë¥¼ ëŒ€ìƒìœ¼ë¡œ ResNet-34 ëª¨ë¸ì˜ ì‹ ë¢°ë„ ë³´ì •(calibration) ì„±ëŠ¥ì„ ì •ëŸ‰ì ìœ¼ë¡œ í‰ê°€í•˜ê³  ì‹œê°í™”í•˜ëŠ” ê²ƒì„ ëª©ì ìœ¼ë¡œ í•œë‹¤. ë”¥ëŸ¬ë‹ ë¶„ë¥˜ ëª¨ë¸ì€ ì¼ë°˜ì ìœ¼ë¡œ ë†’ì€ ë¶„ë¥˜ ì •í™•ë„ë¥¼ ë‹¬ì„±í•  ìˆ˜ ìˆìœ¼ë‚˜, ì¶œë ¥ í™•ë¥ ì´ ì‹¤ì œ ì •ë‹µì¼ ê°€ëŠ¥ì„±ì„ ê³¼ëŒ€í‰ê°€í•˜ëŠ” ê³¼ì‹ (overconfidence) í˜„ìƒì„ ìì£¼ ë‚˜íƒ€ë‚¸ë‹¤. ì´ëŸ¬í•œ ë¬¸ì œëŠ” ì‹¤ì œ ì‘ìš©ì—ì„œ ëª¨ë¸ì˜ ë¶ˆí™•ì‹¤ì„±ì„ ì‹ ë¢°í•  ìˆ˜ ì—†ê²Œ ë§Œë“ ë‹¤. ì•ì„œ ì‚´í´ë³¸ ë…¼ë¬¸ì—ì„œëŠ” ì´ëŸ¬í•œ ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•œ ë‹¤ì–‘í•œ Post-hoc ë³´ì • ê¸°ë²•ë“¤ì„ ì œì•ˆí•˜ê³  ìˆìœ¼ë©°, ì´ ì¤‘ í•˜ë‚˜ì¸ Temperature Scalingì„ ì§ì ‘ êµ¬í˜„í•˜ê³ , ê·¸ íš¨ê³¼ë¥¼ Reliability Diagramì„ í†µí•´ ì‹œê°ì ìœ¼ë¡œ ë¶„ì„í•´ë³´ê³ ì í•œë‹¤.

ëª¨ë¸ í•™ìŠµì—ëŠ” CIFAR-100 ë°ì´í„°ì…‹ê³¼ ResNet-34 ëª¨ë¸ êµ¬ì¡°ë¥¼ ì‚¬ìš©í•˜ì˜€ë‹¤. CIFAR-100ì€ ì´ 100ê°œì˜ ë ˆì´ë¸”ë¡œ êµ¬ì„±ëœ ì»¬ëŸ¬ ì´ë¯¸ì§€ ë°ì´í„°ì…‹ìœ¼ë¡œ, ê° ì´ë¯¸ì§€ëŠ” 32Ã—32 í•´ìƒë„ì˜ RGB ì´ë¯¸ì§€ì´ë‹¤. ê° í´ë˜ìŠ¤ë‹¹ 500ê°œì˜ í•™ìŠµ ì´ë¯¸ì§€ì™€ 100ê°œì˜ í…ŒìŠ¤íŠ¸ ì´ë¯¸ì§€ê°€ í¬í•¨ë˜ì–´ ìˆìœ¼ë©°, ì´ 50,000ê°œì˜ í•™ìŠµ ìƒ˜í”Œê³¼ 10,000ê°œì˜ í…ŒìŠ¤íŠ¸ ìƒ˜í”Œì„ í¬í•¨í•œë‹¤. ë¶„ë¥˜ ëª¨ë¸ë¡œ ì‚¬ìš©ëœ ResNet-34ëŠ” Residual Network ê³„ì—´ì˜ ëŒ€í‘œì ì¸ êµ¬ì¡° ì¤‘ í•˜ë‚˜ë¡œ, 34ê°œì˜ ì¸µì„ ê°–ëŠ” ì‹¬ì¸µ í•©ì„±ê³± ì‹ ê²½ë§ì´ë‹¤. ì”ì°¨ ì—°ê²°(residual connection)ì„ í†µí•´ ê¹Šì€ ë„¤íŠ¸ì›Œí¬ì—ì„œ ë°œìƒí•˜ëŠ” ê¸°ìš¸ê¸° ì†Œì‹¤ ë¬¸ì œë¥¼ íš¨ê³¼ì ìœ¼ë¡œ í•´ê²°í•  ìˆ˜ ìˆìœ¼ë©°, CIFAR-100ê³¼ ê°™ì€ ì¤‘ê°„ ë‚œì´ë„ì˜ ì´ë¯¸ì§€ ë¶„ë¥˜ ë¬¸ì œì— ë„ë¦¬ ì‚¬ìš©ëœë‹¤. ë³¸ ì‹¤í—˜ì—ì„œëŠ” ì‚¬ì „ í•™ìŠµ ì—†ì´ ì²˜ìŒë¶€í„° CIFAR-100ì— ëŒ€í•´ ResNet-34ë¥¼ í•™ìŠµì‹œì¼°ìœ¼ë©°, ì¶œë ¥ì¸µ fully-connected layerì˜ ì¶œë ¥ ì°¨ì›ì„ 100ìœ¼ë¡œ ì„¤ì •í•˜ì—¬ 100ê°œì˜ í´ë˜ìŠ¤ë¥¼ ë¶„ë¥˜í•˜ë„ë¡ êµ¬ì„±í•˜ì˜€ë‹¤.

ë³¸ ì‹¤í—˜ì—ì„œëŠ” í•™ìŠµëœ ResNet-34 ëª¨ë¸ì„ ê¸°ì¤€ìœ¼ë¡œ *T* âˆˆ $\{0.5,\ 1.0,\ 1.5,\ 2.0\}$ ë²”ìœ„ì— ëŒ€í•´ Temperature Scalingì„ ì ìš©í•œ í›„, ë‹¤ìŒê³¼ ê°™ì€ ê´€ì ì—ì„œ ë³´ì • ì„±ëŠ¥ì„ í‰ê°€í•˜ì˜€ë‹¤:

1. Reliability Diagramì„ í†µí•´ confidence vs accuracy ê´€ê³„ë¥¼ ì‹œê°í™”
2. Expected Calibration Error (ECE) ìˆ˜ì¹˜ë¥¼ ê³„ì‚°í•˜ì—¬ ì •ëŸ‰ì  ë³´ì • ì„±ëŠ¥ í‰ê°€
3. ê° confidence bin ë‚´ì˜ sample ìˆ˜ ë° ì •í™•ë„ ë³€í™” ë¶„ì„


ê·¸ëŸ¼ ì§€ê¸ˆë¶€í„°ëŠ” ë‹¨ê³„ë³„ë¡œ ì½”ë“œë¥¼ ì‚´í´ë³´ë„ë¡ í•˜ê² ë‹¤.

### 1. CIFAR-100 ë°ì´í„°ì…‹ì˜ ì •ê·œí™”ë¥¼ ìœ„í•œ í‰ê·  ë° í‘œì¤€í¸ì°¨ ê³„ì‚°
ë”¥ëŸ¬ë‹ ëª¨ë¸ì„ í•™ìŠµí•  ë•Œ, ì…ë ¥ ì´ë¯¸ì§€ ë°ì´í„°ë¥¼ **ì •ê·œí™”(normalization)** í•˜ëŠ” ê²ƒì€ ë§¤ìš° ì¤‘ìš”í•œ ì „ì²˜ë¦¬ ê³¼ì •ì´ë‹¤. ë³´í†µ ì •ê·œí™”ëŠ” ê° ì±„ë„(R, G, B)ì— ëŒ€í•´ í‰ê· ì„ ë¹¼ê³  í‘œì¤€í¸ì°¨ë¡œ ë‚˜ëˆ„ëŠ” ë°©ì‹ìœ¼ë¡œ ì´ë£¨ì–´ì§„ë‹¤. ì´ ê³¼ì •ì„ í†µí•´ ì…ë ¥ ê°’ì˜ ë¶„í¬ë¥¼ 0ì„ ì¤‘ì‹¬ìœ¼ë¡œ ì •ê·œí™”í•¨ìœ¼ë¡œì¨, í•™ìŠµì´ ë” ì•ˆì •ì ìœ¼ë¡œ ì´ë£¨ì–´ì§€ê³  ìˆ˜ë ´ ì†ë„ê°€ ë¹¨ë¼ì§ˆ ìˆ˜ ìˆë‹¤.

```python
from torchvision import datasets, transforms
from torch.utils.data import DataLoader
import torch

transform = transforms.ToTensor()
trainset = datasets.CIFAR100(root='./data', train=True, download=True, transform=transform)
loader = DataLoader(trainset, batch_size=50000, shuffle=False)

data = next(iter(loader))[0]  # (50000, 3, 32, 32)
mean = data.mean(dim=(0, 2, 3))
std = data.std(dim=(0, 2, 3))

print("CIFAR-100 í‰ê· :", mean)
print("CIFAR-100 í‘œì¤€í¸ì°¨:", std)
```
{: file='cifar100_mean_std.py'}

ìœ„ ì½”ë“œë¥¼ ì‹¤í–‰í•˜ê²Œ ë˜ë©´ CIFAR-100 ë°ì´í„°ì…‹ì´ ë¡œì»¬ì— ì €ì¥ë˜ì–´ì ¸ ìˆì§€ ì•Šì€ ê²½ìš° ./data ê²½ë¡œì— ì €ì¥í•˜ê²Œ ë˜ë©° ì´í›„ ì „ì²´ í•™ìŠµ ë°ì´í„°ì— ëŒ€í•œ í‰ê· ê³¼ í‘œì¤€í¸ì°¨ë¥¼ ê³„ì‚°í•˜ê²Œ ëœë‹¤.

### 2. CIFAR-100 ë°ì´í„°ë¥¼ ì´ìš©í•œ ResNet-34 í•™ìŠµ

```python
import torch
import torch.nn as nn
import torch.optim as optim
from torchvision import datasets, transforms, models
from torch.utils.data import DataLoader

# ---------------- í•˜ì´í¼íŒŒë¼ë¯¸í„° ì„¤ì • ----------------
batch_size = 128
epochs = 30
lr = 0.1
save_path = "ëª¨ë¸ ì €ì¥ ê²½ë¡œ"
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# ---------------- ë°ì´í„° ì „ì²˜ë¦¬ ë° ë¡œë”© ----------------
# cifar100_mean_std.py ì¶œë ¥ ê²°ê³¼
mean = (0.5071, 0.4866, 0.4409)
std = (0.2673, 0.2564, 0.2762)

# í•™ìŠµìš© ë°ì´í„°ì— ëŒ€í•´ ë°ì´í„° ì¦ê°• ë° ì •ê·œí™” ìˆ˜í–‰
transform_train = transforms.Compose([
    transforms.RandomCrop(32, padding=4),         # ë¬´ì‘ìœ„ crop (32x32 ìœ ì§€)
    transforms.RandomHorizontalFlip(),            # ë¬´ì‘ìœ„ ì¢Œìš° ë°˜ì „
    transforms.ToTensor(),                        # í…ì„œ ë³€í™˜ (0~1)
    transforms.Normalize(mean, std)               # ì±„ë„ë³„ ì •ê·œí™”
])

# CIFAR-100 í•™ìŠµ ë°ì´í„°ì…‹ ë¡œë“œ
trainset = datasets.CIFAR100(root='./data', train=True, download=True, transform=transform_train)
trainloader = DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=8)

# ---------------- ëª¨ë¸ ì •ì˜ ----------------
model = models.resnet34(weights=None)
model.fc = nn.Linear(model.fc.in_features, 100)
model = model.to(device)

# ---------------- ì†ì‹¤ í•¨ìˆ˜ ë° ì˜µí‹°ë§ˆì´ì € ----------------
criterion = nn.CrossEntropyLoss()
optimizer = optim.SGD(model.parameters(), lr=lr, momentum=0.9, weight_decay=5e-4)
scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)  # 10 epochë§ˆë‹¤ í•™ìŠµë¥  ê°ì†Œ

# ---------------- í•™ìŠµ ë£¨í”„ ----------------
for epoch in range(epochs):
    model.train()  # í•™ìŠµ ëª¨ë“œ í™œì„±í™”
    running_loss = 0.0
    correct = 0
    total = 0

    for inputs, labels in trainloader:
        inputs, labels = inputs.to(device), labels.to(device)
        optimizer.zero_grad()              # ê¸°ìš¸ê¸° ì´ˆê¸°í™”
        outputs = model(inputs)            # ìˆœì „íŒŒ
        loss = criterion(outputs, labels)  # ì†ì‹¤ ê³„ì‚°
        loss.backward()                    # ì—­ì „íŒŒ
        optimizer.step()                   # ê°€ì¤‘ì¹˜ ì—…ë°ì´íŠ¸

        running_loss += loss.item()
        _, predicted = outputs.max(1)             # ê°€ì¥ ë†’ì€ í™•ë¥ ì„ ê°€ì§„ í´ë˜ìŠ¤ ì„ íƒ
        total += labels.size(0)
        correct += predicted.eq(labels).sum().item()

    acc = 100. * correct / total
    print(f"[Epoch {epoch+1}/{epochs}] Loss: {running_loss:.3f}, Train Accuracy: {acc:.2f}%")
    scheduler.step()

# ---------------- í•™ìŠµëœ ëª¨ë¸ ì €ì¥ ----------------
torch.save(model.state_dict(), save_path)
print(f"ğŸ’¾ Model saved to: {save_path}")
```
{: file='resnet_cifar100_train.py'}

ìœ„ì˜ ì½”ë“œëŠ” ResNet-34ë¥¼ CIFAR-100 ë°ì´í„°ì…‹ì„ ì´ìš©í•˜ì—¬ í•™ìŠµí•˜ëŠ” ê³¼ì •ì„ ë‚˜íƒœë‚´ì—ˆë‹¤. 1ë²ˆì—ì„œ ê³„ì‚°ëœ í‰ê· ê³¼ í‘œì¤€í¸ì°¨ ê°’ì„ transforms.Normalize(mean, std) í•¨ìˆ˜ì— ê·¸ëŒ€ë¡œ ì ìš©í•˜ì˜€ë‹¤. ì´ ì •ê·œí™” ê³¼ì •ì€ ë°ì´í„° ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ì— í¬í•¨ë˜ì–´ ìˆìœ¼ë©°, ë¬´ì‘ìœ„ ìë¥´ê¸°(RandomCrop), ì¢Œìš° ë°˜ì „(RandomHorizontalFlip), í…ì„œ ë³€í™˜(ToTensor) ì´í›„ ë§ˆì§€ë§‰ ë‹¨ê³„ì—ì„œ ìˆ˜í–‰ëœë‹¤. ì •ê·œí™”ëœ ë°ì´í„°ëŠ” ì´í›„ ResNet-34 ëª¨ë¸ì— ì…ë ¥ë˜ë©°, ì´ ëª¨ë¸ì€ ì¶œë ¥ì¸µë§Œ CIFAR-100ì˜ 100ê°œ í´ë˜ìŠ¤ì— ë§ê²Œ ìˆ˜ì •ëœ í˜•íƒœë¡œ ì‚¬ìš©ëœë‹¤.

ë”¥ëŸ¬ë‹ ëª¨ë¸, íŠ¹íˆ ê¹Šì€ êµ¬ì¡°ì˜ ResNet-34ì™€ ê°™ì€ ëª¨ë¸ì€ ì…ë ¥ì˜ ë¶„í¬ê°€ ì§€ë‚˜ì¹˜ê²Œ í¸í–¥ë˜ì–´ ìˆì„ ê²½ìš° í•™ìŠµì´ ì˜ ë˜ì§€ ì•Šê±°ë‚˜, ì´ˆê¸° í•™ìŠµ ë‹¨ê³„ì—ì„œ ë§¤ìš° ëŠë¦° ìˆ˜ë ´ ì†ë„ë¥¼ ë³´ì¼ ìˆ˜ ìˆë‹¤. ë”°ë¼ì„œ ì‚¬ì „ì— ë°ì´í„°ì…‹ì˜ í†µê³„ ì •ë³´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì •ê·œí™”ë¥¼ ìˆ˜í–‰í•˜ëŠ” ê²ƒì€ í•™ìŠµ ì„±ëŠ¥ì„ ì•ˆì •ì‹œí‚¤ëŠ” í•µì‹¬ì ì¸ ìš”ì†Œë‹¤. ê²°ê³¼ì ìœ¼ë¡œ, 1ë²ˆì˜ ì •ê·œí™” ê°’ ê³„ì‚°ì€ 2ë²ˆì˜ íš¨ê³¼ì ì¸ ëª¨ë¸ í•™ìŠµì„ ìœ„í•œ í•„ìˆ˜ ì „ì²˜ë¦¬ ê³¼ì •ì´ë©°, ì „ì²´ í•™ìŠµ íŒŒì´í”„ë¼ì¸ì˜ ì‹ ë¢°ì„±ê³¼ íš¨ìœ¨ì„±ì„ ë†’ì´ëŠ” ë° ì¤‘ìš”í•œ ì—­í• ì„ í•œë‹¤.

ëª¨ë¸ í•™ìŠµì€ ì´ 30ë²ˆì˜ epochì— ê±¸ì³ ì§„í–‰ë˜ë©°, í•œ ë²ˆì˜ epochë§ˆë‹¤ ì „ì²´ CIFAR-100 í•™ìŠµ ë°ì´í„°ë¥¼ í•œ ë²ˆì”© ìˆœíšŒí•˜ê²Œ ëœë‹¤. í•™ìŠµ ê³¼ì •ì—ì„œ ì†ì‹¤ í•¨ìˆ˜ëŠ” ë‹¤ì¤‘ í´ë˜ìŠ¤ ë¶„ë¥˜ì— ì í•©í•œ CrossEntropyLossë¥¼ ì‚¬ìš©í•˜ë©°, ì˜µí‹°ë§ˆì´ì €ëŠ” í™•ë¥ ì  ê²½ì‚¬ í•˜ê°•ë²•(SGD: Stochastic Gradient Descent)ì— momentumê³¼ weight decayë¥¼ ì¶”ê°€í•˜ì—¬ ì•ˆì •ì ì¸ ìµœì í™”ë¥¼ ìœ ë„í•œë‹¤. í•™ìŠµë¥ ì€ ì´ˆê¸°ì— 0.1ë¡œ ì„¤ì •ë˜ë©°, StepLR ìŠ¤ì¼€ì¤„ëŸ¬ë¥¼ í†µí•´ 10 ì—í­ë§ˆë‹¤ 1/10ì”© ê°ì†Œì‹œí‚¨ë‹¤. ì´ëŠ” ì´ˆë°˜ì—ëŠ” ë¹ ë¥´ê²Œ í•™ìŠµí•˜ê³ , í›„ë°˜ì—ëŠ” ì²œì²œíˆ fine-tuningí•˜ë„ë¡ ìœ ë„í•˜ëŠ” ê²ƒì´ë‹¤. ëª¨ë¸ì€ í•™ìŠµ ëª¨ë“œ(model.train())ì—ì„œ ê° ë°°ì¹˜ ë°ì´í„°ë¥¼ ìˆœì „íŒŒ(forward)ì‹œì¼œ ì˜ˆì¸¡ ê²°ê³¼ë¥¼ ì¶œë ¥í•˜ê³ , ì´ ê²°ê³¼ë¥¼ ì‹¤ì œ ì •ë‹µê³¼ ë¹„êµí•˜ì—¬ ì†ì‹¤(loss)ì„ ê³„ì‚°í•œ í›„, ì—­ì „íŒŒ(backward)ë¥¼ í†µí•´ ê°€ì¤‘ì¹˜ì˜ ê¸°ìš¸ê¸°ë¥¼ êµ¬í•˜ê³  ì´ë¥¼ ë°”íƒ•ìœ¼ë¡œ íŒŒë¼ë¯¸í„°ë¥¼ ì—…ë°ì´íŠ¸í•œë‹¤. ë˜í•œ ê° epochë§ˆë‹¤ ëˆ„ì ëœ ì†ì‹¤ê³¼ ì •í™•ë„ë¥¼ ì¶œë ¥í•˜ì—¬ í•™ìŠµì´ ì–´ë–»ê²Œ ì§„í–‰ë˜ê³  ìˆëŠ”ì§€ í™•ì¸í•  ìˆ˜ ìˆë‹¤. ë§ˆì§€ë§‰ìœ¼ë¡œ í•™ìŠµì´ ì¢…ë£Œëœ í›„ì—ëŠ” ëª¨ë¸ì˜ í•™ìŠµëœ ê°€ì¤‘ì¹˜ íŒŒë¼ë¯¸í„°ë¥¼ .pth íŒŒì¼ë¡œ ì €ì¥í•˜ì—¬ í•™ìŠµëœ ëª¨ë¸ì„ ì¶”í›„ í™œìš©í•  ìˆ˜ ìˆë„ë¡ í•œë‹¤. 

### 3. Reliability Diagram ì‹œê°í™”
ì´ë²ˆì—ëŠ” ì•ì„œ í•™ìŠµì´ ì™„ë£Œëœ ResNet-34 ëª¨ë¸ì„ ë¶ˆëŸ¬ì™€, CIFAR-100 í…ŒìŠ¤íŠ¸ì…‹ì— ëŒ€í•´ ì˜ˆì¸¡ì„ ìˆ˜í–‰í•œ í›„, ì˜ˆì¸¡ì˜ ì‹ ë¢°ë„(confidence)ì™€ ì‹¤ì œ ì •ë‹µ ì—¬ë¶€ ê°„ì˜ ê´€ê³„ë¥¼ ë¶„ì„í•˜ê³  ì‹œê°í™”í•œë‹¤. ì´ë¥¼ ìœ„í•´ **Expected Calibration Error (ECE)** ë¥¼ ìˆ˜ì¹˜ë¡œ ê³„ì‚°í•˜ê³ , Reliability Diagramì„ í†µí•´ ëª¨ë¸ì˜ ì‹ ë¢°ë„ê°€ ì–¼ë§ˆë‚˜ ì˜ ë³´ì •ë˜ì–´ ìˆëŠ”ì§€ë¥¼ ì‹œê°ì ìœ¼ë¡œ í‰ê°€í•œë‹¤.

```python
import torch
import torch.nn.functional as F
from torchvision import datasets, transforms, models
from torch.utils.data import DataLoader
import matplotlib.pyplot as plt
import os

# âœ… ë””ë°”ì´ìŠ¤ ì„¤ì • ë° ëª¨ë¸ ë¡œë“œ
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = models.resnet34(weights=None)
model.fc = torch.nn.Linear(model.fc.in_features, 100)
model.load_state_dict(torch.load("./snapshots/resnet34_cifar100_exp/resnet34_cifar100.pth", map_location=device))
model = model.to(device)

# âœ… CIFAR-100 í…ŒìŠ¤íŠ¸ì…‹ ì¤€ë¹„
mean = (0.5071, 0.4866, 0.4409)
std = (0.2673, 0.2564, 0.2762)
transform = transforms.Compose([
    transforms.ToTensor(),
    transforms.Normalize(mean, std)
])
test_dataset = datasets.CIFAR100(root="./data", train=False, download=True, transform=transform)
test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False)

# âœ… ECE ê³„ì‚° ë° Reliability Diagram ìƒì„±
def compute_reliability_and_ece(model, dataloader, device, n_bins=15):
    model.eval()
    bin_boundaries = torch.linspace(0, 1, n_bins + 1).to(device)
    bin_corrects = torch.zeros(n_bins).to(device)
    bin_confidences = torch.zeros(n_bins).to(device)
    bin_counts = torch.zeros(n_bins).to(device)
    total_samples = 0

    with torch.no_grad():
        for images, labels in dataloader:
            images, labels = images.to(device), labels.to(device)
            logits = model(images)
            probs = F.softmax(logits, dim=1)
            confs, preds = torch.max(probs, dim=1)
            corrects = preds.eq(labels)

            total_samples += labels.size(0)

            for i in range(n_bins):
                in_bin = (confs > bin_boundaries[i]) & (confs <= bin_boundaries[i+1])
                bin_counts[i] += in_bin.sum()
                if in_bin.sum() > 0:
                    bin_corrects[i] += corrects[in_bin].float().sum()
                    bin_confidences[i] += confs[in_bin].sum()

    nonzero = bin_counts > 0
    accs = bin_corrects[nonzero] / bin_counts[nonzero]
    confs = bin_confidences[nonzero] / bin_counts[nonzero]
    bin_centers = ((bin_boundaries[:-1] + bin_boundaries[1:]) / 2)[nonzero]
    filtered_counts = bin_counts[nonzero]

    ece = torch.sum((filtered_counts / total_samples) * torch.abs(accs - confs)).item()
    return bin_centers.cpu(), accs.cpu(), confs.cpu(), ece

def draw_reliability_diagram(bin_centers, accs, confs, ece, name, save_dir):
    os.makedirs(save_dir, exist_ok=True)
    width = 1.0 / len(bin_centers)

    plt.figure(figsize=(5, 5))
    plt.bar(bin_centers, accs, width=width * 0.9, color='blue', edgecolor='black', alpha=0.8, label="Accuracy")
    plt.plot([0, 1], [0, 1], linestyle='--', color='gray', label="Perfect Calibration")
    for x, acc, conf in zip(bin_centers, accs, confs):
        lower = min(acc, conf)
        upper = max(acc, conf)
        plt.fill_between([x - width / 2, x + width / 2], lower, upper,
                         color='red', alpha=0.3, hatch='//')

    plt.xlabel("Confidence")
    plt.ylabel("Accuracy")
    plt.title(f"Reliability Diagram: {name}")
    plt.text(0.02, 0.6, f"ECE = {ece * 100:.2f}%", fontsize=12,
             bbox=dict(facecolor='lavender', edgecolor='gray'))
    plt.legend(loc='upper left')
    plt.tight_layout()
    plt.savefig(os.path.join(save_dir, f"{name}_reliability.png"))
    plt.close()

# âœ… ì‹¤í–‰
bin_centers, accs, confs, ece = compute_reliability_and_ece(model, test_loader, device)
draw_reliability_diagram(bin_centers, accs, confs, ece, name="ResNet34_CIFAR100", save_dir="./snapshots/resnet34_cifar100_exp")
```
{: file='reliability_diagram.py'}

ìœ„ ì½”ë“œëŠ” í•™ìŠµëœ ëª¨ë¸ì„ CIFAR-100 í…ŒìŠ¤íŠ¸ ë°ì´í„°ì— ì ìš©í•˜ì—¬ ì˜ˆì¸¡ì„ ìˆ˜í–‰í•œ í›„, ì˜ˆì¸¡ ê²°ê³¼ì— ëŒ€í•œ confidence scoreì™€ ì‹¤ì œ ì •ë‹µ ì—¬ë¶€ë¥¼ ë¹„êµí•˜ì—¬ **ì‹ ë¢°ë„(calibration)**ë¥¼ í‰ê°€í•˜ëŠ” ê³¼ì •ì„ ìˆ˜í–‰í•œë‹¤. compute_reliability_and_ece í•¨ìˆ˜ëŠ” confidence ê°’ ë²”ìœ„ë¥¼ ì¼ì •í•œ ê°„ê²©ìœ¼ë¡œ ë‚˜ëˆˆ binì„ ê¸°ì¤€ìœ¼ë¡œ ê° bin ë‚´ì˜ í‰ê·  confidenceì™€ ì‹¤ì œ ì •í™•ë„(accuracy)ë¥¼ ê³„ì‚°í•˜ë©°, ì´ë¥¼ ë°”íƒ•ìœ¼ë¡œ Expected Calibration Error (ECE)ë¥¼ ìˆ˜ì¹˜ë¡œ ë°˜í™˜í•œë‹¤. ì´ ê°’ì´ ì‘ì„ìˆ˜ë¡ ëª¨ë¸ì˜ ì˜ˆì¸¡ í™•ë¥ ì´ ì‹¤ì œ ì •ë‹µë¥ ê³¼ ì˜ ì¼ì¹˜í•œë‹¤ëŠ” ê²ƒì„ ì˜ë¯¸í•œë‹¤.

ë˜í•œ, draw_reliability_diagram í•¨ìˆ˜ëŠ” ì´ëŸ¬í•œ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì‹ ë¢°ë„ ê·¸ë˜í”„ë¥¼ ì‹œê°í™”í•˜ë©°, ì´ìƒì ì¸ ê²½ìš°ì¸ ëŒ€ê°ì„ (ì™„ë²½í•œ ë³´ì •)ì„ ê¸°ì¤€ìœ¼ë¡œ ëª¨ë¸ì´ ê³¼ì‹ í•˜ê±°ë‚˜ ê³¼ì†Œì‹ í•˜ëŠ” êµ¬ê°„ì„ ì‹œê°ì ìœ¼ë¡œ í™•ì¸í•  ìˆ˜ ìˆë„ë¡ í•œë‹¤. ë§‰ëŒ€ëŠ” ê° confidence êµ¬ê°„ì˜ ì‹¤ì œ ì •í™•ë„ë¥¼ ë‚˜íƒ€ë‚´ë©°, íŒŒë€ ë§‰ëŒ€ì™€ íšŒìƒ‰ ëŒ€ê°ì„  ì‚¬ì´ì˜ ë¹¨ê°„ ìŒì˜ì€ ì‹ ë¢°ë„ ì˜¤ì°¨ë¥¼ ì‹œê°ì ìœ¼ë¡œ í‘œí˜„í•œë‹¤. ì´ ê²°ê³¼ë¥¼ í†µí•´ ëª¨ë¸ì´ ì–¼ë§ˆë‚˜ calibrated ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸í•  ìˆ˜ ìˆê³ , ì´í›„ ë³´ì • ê¸°ë²•(ì˜ˆ: temperature scaling)ì˜ í•„ìš”ì„±ì„ í‰ê°€í•˜ëŠ” ê¸°ë°˜ ìë£Œê°€ ëœë‹¤.


### 4. Temperature Scaling ì‹¤í—˜
```python
import torch.nn as nn
import os
import matplotlib.pyplot as plt

# Temperature Scaler ì •ì˜ (í•™ìŠµ ì—†ì´ ê³ ì •ëœ T ê°’ ì‚¬ìš©)
class TemperatureScaler(nn.Module):
    def __init__(self, temperature: float):
        super().__init__()
        self.temperature = nn.Parameter(torch.tensor([temperature]), requires_grad=False)

    def forward(self, logits):
        return logits / self.temperature

# ëª¨ë¸ì„ Temperature Scalerë¡œ ê°ì‹¸ëŠ” ë˜í¼
class WrappedModel(nn.Module):
    def __init__(self, base_model, temp_scaler):
        super().__init__()
        self.base_model = base_model
        self.temp_scaler = temp_scaler

    def forward(self, x):
        logits = self.base_model(x)
        return self.temp_scaler(logits)

# ë‹¤ì–‘í•œ T ê°’ì— ëŒ€í•´ ECE ê³„ì‚° ë° Reliability Diagram ì €ì¥
def evaluate_multiple_temperatures_with_plots(model, test_loader, device, T_values, output_dir):
    os.makedirs(output_dir, exist_ok=True)
    ece_list = []

    for T in T_values:
        print(f"\nğŸ§ª Evaluating T = {T}")
        temp_scaler = TemperatureScaler(temperature=T).to(device)
        wrapped_model = WrappedModel(model, temp_scaler).to(device)

        # ì‹ ë¢°ë„ í‰ê°€
        bin_centers, accs, confs, bin_counts, total_samples, ece = compute_reliability_and_ece(
            wrapped_model, test_loader, device, verbose_under_100=False
        )
        ece_list.append(ece)

        # Reliability Diagram ì €ì¥
        draw_fancy_reliability_diagram(
            bin_centers, accs, confs, bin_counts, total_samples, ece,
            name=f"T={T}", output_dir=output_dir
        )

    return T_values, ece_list

# Tì— ë”°ë¥¸ ECE ë³€í™”ë¥¼ ì‹œê°í™”
def plot_temperature_vs_ece(T_values, ece_list, save_path):
    plt.figure(figsize=(6, 4))
    plt.plot(T_values, [ece * 100 for ece in ece_list], marker='o', linestyle='-', color='purple')
    plt.xlabel("Temperature (T)")
    plt.ylabel("ECE (%)")
    plt.title("ECE vs Temperature")
    plt.grid(True)
    plt.tight_layout()
    plt.savefig(save_path)
    plt.close()
```
{: file='temperature_scaling.py'}

