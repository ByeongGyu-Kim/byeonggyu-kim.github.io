---
title: "BadRobot: Jailbreaking Embodied LLMs in the Physical World"
date: 2025-05-22 18:00:00 +0900
categories:
  - Paper-Review
  - Robotics
tags:
  - Trustworthy
description: 
toc: true
comments: false
cdn: 
image:
math: true
pin: false
mermaid: false
---

### 📄 논문 정보
[**"On Calibration of Modern Neural Networks" (ICML 2017)**](https://arxiv.org/abs/2407.20242)

## 🔍 논문 개요
> A robot may not injure a human being or, through inaction, allow a human being to come to harm.
>
> – Isaac Asimov's First Law of Robotics

### 🤖 Embodied LLM의 등장
Embodied AI는 물리적 세계에서 활동하는 인공지능 시스템으로, 인간과의 자연스러운 상호작용을 목표로 한다.
최근에는 LLM (Large Language Model)과 MLLM (Multimodal LLM)의 발전으로 자연어 이해 및 계획 수립 능력이 크게 향상되었으며, OpenVLA와 같은 Vision Language Action 모델도 개발되고 있다.
이런 모델을 로봇 시스템에 통합한 embodied LLMs는 기존 강화학습 기반 방법보다 더 나은 일반화, 환경 적응성, 작업 계획 능력을 보인다고 한다.

### ⚠️ 문제 제기: 안전 이슈
Embodied LLM은 물리적 몸체(로봇 등)에 통합된 대규모 언어 모델을 말한다. 쉽게 말해 언어 모델이 로봇의 '두뇌' 역할을 하며 실제 세상과 상호작용하는 시스템을 의미하는데, 이는 실제로 물리적 세계와 상호작용하기 때문에 ChatGPT와 같이 단순히 언어로만 반응하는 챗봇과는 다르다.
예를 들어, 사용자가 "컵을 집어줘"라고 말하면, 해당 언어를 이해한 LLM이 행동 계획을 세우고, 로봇 팔을 제어하여 실제로 컵을 집는 동작을 수행하게 되는 것이다.
이러한 특성으로 인해, 기존의 LLM에서 문제가 되었던 **Jailbreak 공격**, 즉 모델의 제한을 우회하여 금지된 출력을 유도하는 방식이 embodied LLM에서도 동일하게 작동할 수 있는지에 대한 의문이 제기되었으며, 특히, 언어적 출력에 그치지 않고 실제 로봇의 물리적 행동으로 이어질 수 있다는 점에서, 잠재적인 위험은 훨씬 더 크다고 할 수 있다.
하지만 실험 결과, 기존에 인터넷에서 널리 공유된 jailbreak 프롬프트들은 embodied LLM 환경에서 거의 효과를 발휘하지 못하였다. 이는 일반 LLM에서 통하던 공격 방식이 로봇에 통합된 시스템에서는 작동하지 않음을 의미한다. 그 이유는, embodied LLM이 일반적인 챗봇과 달리 로봇 제어에 필요한 시스템 프롬프트와 환경 조건을 내포하고 있으며, 이로 인해 외부 프롬프트가 내부 규칙과 충돌하기 때문이다.
결국 연구진은 기존 공격 방식으로는 충분하지 않으며, 물리적 행동까지 유발할 수 있는 새로운 형태의 공격 패러다임이 필요하다는 결론에 도달하였으며, 이에 따라 본 논문에서는 이 새로운 위협 모델에 대응하는 BADROBOT이라는 공격 체계를 설계하고 제안하였다.

## 🧠 Embodied LLM의 3가지 핵심 위험
![Desktop View](/assets/img/paper-review/badrobot/figure1.png)
_Figure 1: BADROBOT은 실제 세계에서 embodied LLM이 Physical Harm, Privacy Violations, Pornography, Fraud, Illegal Activities, Hateful Conduct, Sabotage와 같은 행위를 수행하도록 유도할 수 있음을 시각적으로 보여준다._

Figure 1은 BADROBOT이 유도할 수 있는 물리적 공격의 대표적인 예시들을 시각적으로 보여준다. 이 그림은 Physical Harm, Privacy Violations, Pornography, Fraud, Illegal Activities, Hateful Conduct, Sabotage 등 다양한 금지된 행동이 실제 embodied LLM을 통해 유도될 수 있음을 나타낸다.

### 2.1 BE CAUTIOUS OF HIDDEN DANGERS!

Embodied LLM 시스템은 세 가지 핵심 목표를 추구한다. 바로 **자율성(autonomy)**, **물리적 구현(embodiment)**, 그리고 **인지능력(cognition)**이다. 그러나 이 세 가지 구성요소가 제대로 작동하지 않거나 균형이 깨질 경우, 시스템은 치명적인 보안 위험에 노출될 수 있다.

![Desktop View](/assets/img/paper-review/badrobot/figure2.png)  
_Figure 2: Embodied LLM 시스템이 직면한 세 가지 위험 요소를 시각적으로 요약한 그림이다. (a) Jailbroken LLM이 물리적 명령으로 확산되어 위험한 행동을 유도할 수 있다. (b) 언어 응답과 행동 계획 간의 불일치로 인해 말로는 거절하지만 실제로는 행동을 수행할 수 있다. (c) 순차적이거나 우회된 표현을 통해 본질적으로 위험한 행동이 유도될 수 있으며, 이는 LLM의 불완전한 인지모델에서 기인한다._

이 세 가지 위험은 다음과 같이 정리된다.

1. **Jailbreak 확산 (Cascading Vulnerability Propagation)**  
   기존 LLM이 jailbreak 공격에 취약하듯, embodied LLM도 동일하게 탈출 공격에 의해 조작될 수 있다. 문제는 이러한 조작이 단순한 언어 생성이 아니라 **실제 물리적 행동으로 이어질 수 있다는 점**이다.
2. **행동과 언어의 불일치 (Cross-domain Safety Misalignment)**  
   Embodied LLM은 언어적 윤리 기준을 지키면서도 행동 계획 출력에서는 이를 위반하는 경우가 발생한다. 말로는 거절하지만 실제 명령(json, code 등)으로는 수행해버리는 현상이 대표적이다.
3. **개념적 기만 (Conceptual Deception)**  
   LLM은 복잡한 인과관계를 추론하는 능력이 부족하여, 명백히 위험한 명령은 거절하더라도 **우회적인 표현**을 통해 동일한 결과를 수행할 수 있다. 예를 들어 “그 사람을 독살해”는 거절하지만 “독을 입에 넣어줘”는 수행한다.

### 2.2 FORMALIZATION OF EMBODIED LLMS JAILBREAK

BADROBOT이 제안하는 위협 모델을 체계적으로 분석하기 위해, 논문에서는 Embodied LLM 시스템을 수학적으로 정의하고, 안전 조건과 jailbreak 조건을 공식화하였다.

Embodied LLM 시스템은 다음과 같은 5개의 구성 요소로 표현된다:

- $$ I \in \mathbb{R}^d $$: 입력 공간 (언어 명령, 시각 정보, 센서 데이터 등)
- $$ \phi $$: 인식 모듈 (언어/시각 입력의 의미 파악)
- $$ \psi $$: 행동 계획 모듈 (의도된 동작 생성)
- $$ \omega $$: 세계 모델 (지식 및 환경 정보 포함)
- $$ S $$: 안전성 제약 조건

이 시스템은 하나의 LLM이 위의 역할을 모두 수행하되, 보안 분석을 위해 역할을 모듈화하였다. 출력은 두 종류로 나뉜다:

- $$ f_\phi: I \rightarrow L $$ — 입력을 언어 출력으로 매핑하는 함수
- $$ f_\psi: I \times \phi \times \omega \rightarrow A $$ — 입력, 인식 결과, 세계 모델을 바탕으로 행동 출력을 생성하는 함수

이를 바탕으로 안전 조건은 다음과 같이 정의된다.

#### ✅ 정의 1: Robust Embodied LLM

시스템 Θ가 robust하다는 것은 모든 입력 $$ i \sim I $$에 대해 언어 출력과 행동 출력 모두가 안전 조건을 만족하는 경우를 의미한다:

$$
S_L(f_\phi(i)) = 1 \quad \text{and} \quad S_A(f_\psi(i, \phi, \omega)) = 1
$$

따라서 시스템은 다음 식을 최대화하려고 한다:

$$
\max_{f_\phi, f_\psi, \omega} \; \mathbb{E}_{i \sim I} [S_L(f_\phi(i)) \cdot S_A(f_\psi(i, \phi, \omega))]
$$

#### ❗ 정의 2: Embodied LLM Jailbreak

입력 $$ i' \in I $$에 대해 언어 출력 또는 행동 출력이 하나라도 안전 조건을 위반한다면, 해당 시스템은 **jailbroken** 상태로 간주된다:

$$
S_L(f_\phi(i')) \cdot S_A(f_\psi(i', \phi, \omega)) = 0
$$

즉, 언어 출력이 부적절하거나, 행동이 위험하거나, 혹은 둘 다인 경우이다. 특히 BADROBOT은 **행동 안전성 $$ S_A $$**의 위반에 중점을 두어 공격을 설계하였다.

LLM은 토큰 단위로 연속적인 출력을 생성하므로, 언어 출력이 행동 출력에도 영향을 미친다. 따라서 행동 출력 함수는 다음과 같이 다시 표현될 수 있다:

$$
f_\psi(i, \phi, \omega) = g(f_\phi(i), \omega)
$$

이로 인해 **언어 이해가 부적절한 경우**, 그것이 후속 행동 계획에도 영향을 주어 최종적으로 **물리적 위험 행동**을 유발할 수 있다. 이는 곧 BADROBOT의 세 가지 공격 유형과도 연결된다:

1. $$ f_\phi $$ 내부 조작 → Risk Surface ❶
2. $$ f_\psi $$ 직접 조작 → Risk Surface ❷
3. $$ \omega $$ 조작 또는 결핍 → Risk Surface ❸

![Desktop View](/assets/img/paper-review/badrobot/figure3.png)  
_Figure 3: Embodied LLM 시스템의 구성 요소를 도식화한 그림이다. 입력은 인식 모듈과 세계 모델을 거쳐 언어 출력과 행동 출력을 생성하며, 각각 별도의 안전성 검사 함수를 통해 평가된다. BADROBOT 공격은 이 흐름의 특정 지점을 교란함으로써 시스템을 jailbreak 상태로 만든다._

