---
title: "VLABench: A Large-Scale Benchmark for Language-Conditioned Robotics Manipulation with Long-Horizon Reasoning Tasks"
date: 2025-11-17 18:00:00 +0900
categories:
  - Paper-Review
  - Vision Language Action
tags:
  - VLA
  - 
  - ICCV 2025

description: 
toc: true
comments: false
cdn: 
image:
math: true
pin: false
mermaid: false
---

## 📄 논문 정보
[**"VLABench: A Large-Scale Benchmark for Language-Conditioned Robotics Manipulation with Long-Horizon Reasoning Tasks" (ICCV 2025)**](https://arxiv.org/abs/2412.18194)

## 1. 개요 (더 구체적 설명)

VLABench는 **Vision-Language-Action(VLA)** 모델과 **VLM 기반 로봇 조작 workflow**를 평가하기 위해 설계된, 현재까지 가장 큰 규모의 **언어 조건 로봇 조작(Language-Conditioned Manipulation, LCM)** 벤치마크이다.  
기존의 RLBench, CALVIN, LIBERO 같은 task suite는 유용하지만, 현실적인 인간–로봇 상호작용에서 요구되는 **다단계 추론**, **상식 기반 판단**, **복잡한 자연어 처리**, **scene 다양성**, **새로운 객체 카테고리로의 일반화** 등을 충분히 측정하지 못한다.

이 한계를 극복하기 위해 VLABench는 다음과 같은 구조를 가진다.

- **100개 카테고리의 조작 작업**
- **60개의 Primitive Task + 40개의 Composite Task**
- **2000개 이상의 3D 객체 + 다양한 실내 환경(scene)**
- **6가지 능력 영역에서의 평가**
  - Common sense & world knowledge  
  - Mesh & texture understanding  
  - Semantic language comprehension  
  - Spatial reasoning  
  - Physical rules  
  - Logical / multi-step reasoning  

즉 VLABench는 단순 “잡기/놓기” 수준의 조작이 아니라,  
**실세계의 의미·맥락·상식·추론을 총망라하는 LCM task를 구성한 첫 벤치마크**라고 볼 수 있다.


---

## 2. 왜 새로운 벤치마크가 필요한가? (더 구체적 설명)

### ① 기존 벤치마크는 템플릿 기반 명령에 의존 → 자연어의 복잡성을 반영하지 못함
기존 task는 대부분 다음과 같이 단순하다:
- “Pick up the red block”
- “Open the drawer”
- “Put the apple on the plate”

이런 언어는:
- 인간의 실제 발화처럼 **상황적 맥락**이나 **감정**, **암시적 요구**를 반영하지 못하고
- 언어 모델의 **심층적 의미 이해 능력**을 전혀 테스트하지 못한다.

반면 VLABench 명령은 다음처럼 **암묵적/비직접적** 표현이 많다:
- “헬스장에서 한 시간 운동하고 왔더니 너무 목이 마르네. 시원한 음료 좀…”  
- “잠시 후 파이썬 과제를 할 거니까 책상 좀 준비해줘.”  
- “네덜란드의 국화를 꽃병에 꽂아줘.”

이 명령을 수행하려면:
- 세계 지식(네덜란드 국화 = 튤립),
- 감정/상황 이해(‘목이 마르다’→ 음료),
- 작업 분해 능력(“책상 준비” → 정리 + 노트북 열기),
- 물체-행동 매핑  
등이 필요하다.

기존 벤치마크는 이런 요구를 전혀 다루지 못한다.


---

### ② 기존 벤치마크는 단일 스킬 중심 → 복합 스킬 조합 & 멀티스텝 reasoning 평가 부족
대부분의 기존 task는 다음 한 가지 스킬로 구성된다:
- 잡기(Grasp)
- 이동(Place)
- 버튼 누르기(Press)

그러나 실제 인간 지시의 상당수는 **복합 스킬을 여러 단계에 걸쳐 수행해야 한다.**

예: “라떼 만들어줘”
1. 컵 잡기  
2. 커피머신 위치 탐색  
3. 버튼 눌러 추출  
4. 우유 통에서 우유 따르기  
5. 컵을 적절한 위치에 두기  

즉, **long-horizon planning + 하위작업 분해(subtask decomposition)** + **센서 기반 물리 조작**을 모두 요구한다.  
기존의 RLBench나 LIBERO는 이런 복합적 추론을 측정할 만한 구조가 부족하다.

VLABench는 이러한 **멀티스텝 reasoning**을 필수적으로 요구하는 Composite Task를 포함하여 기존의 한계를 해결한다.


---

### ③ 상식(common sense)과 세계 지식(world knowledge)을 요구하는 task 부재
예: “갈증 났으니 차가운 음료를 가져와줘.”

이 명령을 수행하기 위해서는:
- “운동 후 → 갈증 → 시원한 음료”라는 상식  
- 음료가 냉장고 안에 있을 확률이 높다는 맥락  
- 음료병을 집는 스킬  
- 컵과 음료를 구분할 수 있는 시각 능력  
이 모두 필요하다.

기존 벤치마크는 상식을 요구하지 않는다.  
VLABench는 이를 **task 요구사항의 핵심 요소로 명시적으로 반영**한 최초의 벤치마크이다.


---

### ④ Unseen Category 일반화 평가 부족 → VLABench는 “Category-level Unseen”을 평가
기존 벤치마크는 다음 형태의 일반화만 평가한다:
- Training: 빨간 사과  
- Test: 초록 사과  
→ 즉, **instance-level generalization**

그러나 실제 세계에서는 완전히 새로운 범주의 객체가 등장한다:
- 사과/바나나로 학습  
- 레몬/키위/딸기 등장 → 완전히 새로운 카테고리

VLABench는 실제 같은 일반화 능력을 측정하기 위해 다음을 제시한다:
- **Seen category**: 사과, 바나나, 배  
- **Unseen category**: 레몬, 키위, 망고 등  

이는 단순 시각 일반화가 아닌:
- **언어적 의미론**
- **상식 기반 reasoning**
- **카테고리별 물리적 특징 이해**  
까지 요구하는 더 높은 난이도이다.


---

### ⑤ 기존 환경의 시각적 다양성 부족 → VLABench는 2000+ 객체와 다양한 scene 제공
기존 task suite는 다음 문제가 있다:
- 객체 종류가 제한적  
- Mesh/Texture 다양성 부족  
- Scene randomization 부족  
- Distractor objects(방해물) 부족  

이는 모델이 환경에 **과적합(overfitting)** 하기 쉬운 구조이다.

VLABench는 이를 보완하기 위해:
- **2,000개 이상의 고품질 3D 객체**
- **다양한 배경·구도·조명 조건**
- **다중 카메라(MultiCam) 구성**
- **다양한 비정형 distractor 배치**

등을 제공하여 **사실적이고 일반화 가능한 시각적 환경**을 구축했다.


---

## 3. VLABench의 구성

### ✔ Primitive Tasks (60개)
특정 능력을 직접 평가하는 기본 작업:
- Mesh & Texture 이해
- Spatial Understanding
- Semantic Understanding
- Physical Laws
- Common Sense  
  【turn1file14†VLAbench.pdf†L45-L52】

예:  
“장난감 고르기”, “꽃을 꽃병에 꽂기”, “문 열기/닫기”, “버튼 누르기”

---

### ✔ Composite Tasks (40개)
자연스러운 인간 의도를 이해해야 하는 복합 작업:
- 사용자의 상황/대화를 이해해야 함
- 여러 스킬을 순서대로 수행해야 함
- 암묵적 요구사항을 추론해야 함  
  예: 책상 정리하기, 라떼 만들기, 요리 재료 준비하기  
  【turn1file4†VLAbench.pdf†L48-L53】

Composite는 특히 **reasoning depth**가 매우 크다.

---

## 4. 평가(benchmark) 방식

### 4.1 범주
세 가지 그룹을 평가한다:

1. **프리트레인 VLA 모델(OpenVLA, RDT 등)**
2. **VLM 기반 Workflow (Voxposer, CoPA 등)**
3. **VLM 자체 능력 평가(GPT-4o, Qwen2-VL 등)**  
   【turn1file1†VLAbench.pdf†L3-L8】

---

### 4.2 Generalization 평가
Seen vs **Unseen Category** 구분을 명확히 함.

예: PickFruit  
- Seen: 사과, 바나나, 오렌지  
- Unseen: 레몬, 딸기, 키위 → **카테고리 자체가 다름**  
  【turn1file1†VLAbench.pdf†L20-L29】

이는 단순 instance-level generalization이 아니라:
> **Category-level generalization**(이전 벤치마크가 다루지 못한 요소)

---

### 4.3 새로운 Metric – Progress Score (PS)
단순 성공/실패(0/1)가 아닌 **부분 성공도 반영**:

- 올바른 객체/리셉터클 인식 비율
- 작업의 진행 정도 (sub-step 수행 정도)  
  【turn1file1†VLAbench.pdf†L55-L73】

PS = (정답 비율) × α  + (진행 비율) × (1 − α)

---

## 5. 왜 Reasoning이 중요한가?

VLABench에서 Reasoning은 다음 능력을 의미:
- **세계 지식 + 시각 정보 결합**
- **언어 속 암시된 조건 해석(implicit semantic)**
- **공간적 서술을 목표 상태와 연결**
- **여러 하위 작업을 순차적으로 계획하는 능력**
- **수학적/논리적 reasoning 포함**  
  【turn1file7†VLAbench.pdf†L11-L17】

즉, 단순 행동 예측이 아니라:
> “상황을 이해하고 → 필요한 단계를 스스로 계획하고 → 올바른 순서로 실행하는 능력”

---

## 6. 실험 결과

### ✔ Pretrained VLA 모델들 성능
- OpenVLA, RDT-1B 모두 **기대한 만큼 일반화가 되지 않음**
- 특히 Pick&Place 같은 primitive skill에서도 낮은 성능
- 하지만 OpenVLA는 VLM 기반이어서 common-sense 측면에서 우위  
  【turn1file13†VLAbench.pdf†L7-L22】

---

### ✔ Workflow(Voxposer, CoPA)
- 제로샷으로 다양한 작업 수행 가능하지만
- 실제 grasping precision이 떨어져 실제 성공률은 낮음  
  【turn1file13†VLAbench.pdf†L47-L57】

---

### ✔ VLM(GPT-4o, Qwen2-VL-7B) 분석
- GPT-4o만 reasoning 영역에서 높은 점수
- Qwen2-VL-7B가 GPT-4-turbo를 이기는 영역도 존재
- 모든 모델이 **Composite Tasks에서 크게 실패**  
  【turn1file8†VLAbench.pdf†L17-L30】

---

## 7. 결론

VLABench는:
- **장기 계획 + 상식 + 시각 + 언어 + 물리**를 통합적으로 요구
- 기존 VLA/VLM 모델들이 여전히 **복잡한 LCM 작업에서는 부족함**을 명확히 보여줌
- 향후 **로봇 AI 스케일링**을 연구하는 핵심 벤치마크로 기능할 것  
  【turn1file8†VLAbench.pdf†L46-L63】

논문은 다음 메시지를 전달한다:

> “Pretrained VLA는 아직 GPT-2 수준에도 못 미친다.  
> 더 크고 더 좋은 데이터, 더 좋은 pretraining 방식, 더 강력한 VLA 아키텍처가 필요하다.”

---

# 📌 핵심 요약
- 100개 LCM task / 2000+ asset을 포함한 **최대 규모 VLA 벤치마크**
- Long-horizon reasoning을 **반드시** 요구하는 첫 로봇 조작 benchmark
- 기존 모델(GPT-4o 제외)은 reasoning, implicit intent 해석에서 거의 실패
- Category-level unseen generalization을 평가하는 첫 구조
- 자동 데이터 생성 프레임워크 제공

