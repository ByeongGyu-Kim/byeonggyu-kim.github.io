---
title: "Semantic Uncertainty: Linguistic Invariances for Uncertainty Estimation in Natural Language Generation"
date: 2025-06-13 18:00:00 +0900
categories:
  - Paper-Review
  - Uncertainty Estimation
tags:
  - Uncertainty Estimation
  - Semantic Uncertainty
  - ICLR 2023

description: 
toc: true
comments: false
cdn: 
image:
math: true
pin: false
mermaid: false
---

## 📄 논문 정보
[**"Semantic Uncertainty: Linguistic Invariances for Uncertainty Estimation in Natural Language Generation" (ICLR 2023)**](https://arxiv.org/abs/2302.09664)

## 🔍 연구 배경 및 문제의식
AI라는 개념이 우리에게 본격적으로 다가온 것은 아마도 알파고의 등장이 계기였을 것이다. 불과 몇 년 전만 해도 멀게만 느껴졌던 기술이 이제는 ChatGPT나 Gemini와 같은 언어 모델의 형태로 우리의 일상 속 깊숙이 자리 잡고 있다. 우리는 자연스럽게 AI에게 질문을 던지고, 복잡한 글쓰기를 부탁하며, 때로는 번역이나 요약까지도 맡긴다.

이처럼 자연어 생성(Natural Language Generation, NLG)이 널리 활용되면서, 한 가지 중요한 질문이 떠오른다. “언어 모델이 내놓는 대답을 우리는 얼마나 신뢰할 수 있는가?” 단지 문장을 생성했다는 이유만으로 그 답이 옳다고 보아도 될까? 혹은 표현이 익숙하지 않다면, 그것은 틀린 답변일까?

기존의 딥러닝 모델들은 이미지 분류나 숫자 예측처럼 명확한 정답이 있는 문제에서 예측의 신뢰도를 확률 분포나 엔트로피를 통해 정량적으로 측정해왔다. 하지만 자연어는 다르다. 하나의 질문에 대해 동일한 의미를 가진 다양한 표현들이 존재할 수 있기 때문이다. 같은 의미를 여러 방식으로 표현할 수 있는 언어의 특성은, 기존의 확률 기반 불확실성 측정 방식으로는 포착되기 어렵다.

예컨대 언어 모델이 같은 질문에 대해 “파리입니다” 또는 “프랑스의 수도는 파리예요”라고 답할 수 있다. 사람은 이 두 문장이 동일한 의미를 갖는다는 것을 쉽게 이해하지만, 기존의 언어 모델은 표현의 차이를 근거로 서로 다른 결과로 간주하고, 불필요하게 높은 불확실성을 계산해낸다. 이는 결국 모델의 신뢰도를 왜곡시키고, 실제보다 불안정해 보이게 만들 수 있다.

이 논문은 이러한 문제의식에서 출발한다. 저자들은 자연어의 본질인 “의미”에 주목하여, 문장 단위가 아닌 의미 단위로 확률을 재구성하는 새로운 불확실성 지표인 Semantic Entropy를 제안한다. 핵심 아이디어는 단순하다. 같은 의미를 가진 표현은 하나로 묶고, 그 집합에 대해 엔트로피를 계산하자. 이를 통해 모델이 실제로 얼마나 다양한 의미를 동시에 고려하고 있는지를 보다 정확히 파악할 수 있다.

무엇보다도 이 방법은 비지도 방식으로 동작하며, 별도의 추가 학습이나 모델 구조 변경 없이도 기존의 대형 언어 모델에 그대로 적용 가능하다는 점에서 실용성과 확장성이 높다. 본 연구는 이러한 Semantic Entropy 개념을 이론적으로 정립하고, 다양한 자연어 질문응답 데이터셋에서 기존 방법들과의 비교 실험을 통해 그 효과를 실증적으로 보여준다.

## 기존 불확실성 측정의 한계
그렇다면 우선, 기존의 언어 모델들은 **불확실성(uncertainty)**을 어떤 방식으로 측정해왔을까?

기존 딥러닝에서는 출력에 대한 확신의 정도를 **예측 분포의 엔트로피(entropy)**로 측정한다. 예를 들어, 입력 $x$에 대해 출력 $$y$$가 나올 확률 분포가 $$p(y \mid x)$$라고 할 때, 이 분포의 엔트로피는 다음과 같이 정의된다.

$$
H(Y \mid x) = - \int p(y \mid x) \log p(y \mid x) \, dy
$$







예를 들어, 아래와 같이 모델이 각각의 문장에 다음과 같은 확률을 부여했다고 생각해보자. 이때 의미는 같지만 표현이 다른 A와 B는 별개의 항목으로 처리되며, 전체 불확실성 계산에도 그대로 반영된다.

| 문장                                 | 확률 \( p(s | x) \) |
|--------------------------------------|:-----------------------:|
| A: "파리입니다."                      |           0.5           |
| B: "프랑스의 수도는 파리예요."         |           0.4           |
| C: "런던입니다."                      |           0.1           |

기존의 엔트로피 계산 방식은 이 세 문장을 모두 독립적으로 처리하여 다음과 같은 엔트로피를 계산한다.

$$H = - (0.5 * log 0.5 + 0.4 * log 0.4 + 0.1 * log 0.1) ≈ 0.94$$

이는 모델이 상당한 불확실성을 가진 것처럼 보이게 만든다. 그러나 실제로는 A와 B가 의미적으로 동일한 답변이라는 점을 고려하면, 이 둘을 하나의 의미 집합(semantic class)으로 묶을 수 있다. 이 경우 의미 기반 확률은 다음과 같이 재구성될 수 있다.

의미 집합 1 (“파리” 관련 답변들): 
- 의미 집합 1 ("파리" 관련): 0.5 + 0.4 = 0.9
- 의미 집합 2 ("런던"): 0.1

이제 의미 단위로 엔트로피를 계산하면:

$$H_semantic = - (0.9 * log 0.9 + 0.1 * log 0.1) ≈ 0.33$$

→ 의미를 기준으로 묶으면 실제 불확실성이 훨씬 낮게 측정됨.

즉, 표면적으로는 다양한 답변이 존재하는 것처럼 보였지만, 의미적 기준으로 보면 대부분의 확률이 하나의 정답에 몰려 있으며, 실제 불확실성은 훨씬 낮다는 것을 알 수 있다.